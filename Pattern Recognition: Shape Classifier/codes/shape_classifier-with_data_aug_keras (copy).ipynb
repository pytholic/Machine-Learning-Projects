{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PB Assignment # 01\n",
    "Pattern Recognition\n",
    "\n",
    "Raja Haseeb\n",
    "20194673\n",
    "\n",
    "FOR TAs:\n",
    "\n",
    "- Follow steps if you want to retrain models\n",
    "- I trained multiple models, but KNN provides best results. So you can just comment out rest in train sectionand     just train the KNN to save time\n",
    "- Also you can just jump to bottom to load the provided trained model and just test it to save more time\n",
    "- Only keras augmentation is used for final model. You can skip the manual augmentation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trojan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/trojan/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.feature import canny \n",
    "import skimage\n",
    "from skimage import data, io\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage import color \n",
    "\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists to save the labels (the name of the shape)\n",
    "train_dir = '/home/trojan/Desktop/pattern recognition/PB1/Implementation/data/shapes'\n",
    "save_dir = '/home/trojan/Desktop/pattern recognition/PB1/Implementation'\n",
    "shape_list = ['circle', 'triangle', 'tetragon', 'pentagon', 'other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(featurewise_center=True, rotation_range=10, width_shift_range=0.1,\n",
    "                         height_shift_range=0.1, shear_range=0.15, zoom_range=0.1, \n",
    "                         channel_shift_range=0., horizontal_flip=True, vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def augmentation_keras(dir):\n",
    "    for shape in shape_list:\n",
    "        for file_name in os.listdir(os.path.join(dir,shape)):\n",
    "                PATH = os.path.join(dir,shape)\n",
    "                img = os.path.join(dir,shape,file_name)\n",
    "                image = cv2.imread(img, -1)\n",
    "                image = np.expand_dims(cv2.imread(img), 0)\n",
    "                #image = image.squeeze()\n",
    "                #plt.figure()\n",
    "                #plt.imshow(image)\n",
    "                aug_iter = gen.flow(image, save_to_dir=PATH, save_prefix='aug-image-' + file_name, save_format='png')\n",
    "                aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(100)]\n",
    "                #plotImages(aug_images)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trojan/.local/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "# Run this block to start augmentation\n",
    "\n",
    "augmentation_keras(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In final work I used Keras augmwntation, skip these blocks\n",
    "\n",
    "'''def augmentation(dir, Extension, th1, th2, x, y, deg, deg2, deg3):\n",
    "    for shape in shape_list:\n",
    "        for file_name in os.listdir(os.path.join(dir,shape)):\n",
    "            PATH = os.path.join(dir,shape)\n",
    "            img = os.path.join(dir,shape,file_name)\n",
    "            image = cv2.imread(img, 0)\n",
    "            \n",
    "            # flipping the image\n",
    "            image_xaxis_flipped = cv2.flip(image, 0)\n",
    "            image_yaxis_flipped = cv2.flip(image, 1)\n",
    "            \n",
    "            # Canny edge detect\n",
    "            image_canny = cv2.Canny(image,th1,th2)\n",
    "    \n",
    "            # Translation\n",
    "            rows, cols = image.shape\n",
    "            M_trans = np.float32([[1, 0, x], [0, 1, y]])\n",
    "            image_translated = cv2.warpAffine(image, M_trans, (cols, rows))\n",
    "            \n",
    "            # Rotation\n",
    "            rows, cols = image.shape\n",
    "            M_rot_90 = cv2.getRotationMatrix2D((cols/2,rows/2), deg, 1)\n",
    "            M_rot_180 = cv2.getRotationMatrix2D((cols/2,rows/2), deg2, 1)\n",
    "            M_rot_270 = cv2.getRotationMatrix2D((cols/2,rows/2), deg3, 1)\n",
    "            image_rotated_90 = cv2.warpAffine(image, M_rot_90, (cols, rows))\n",
    "            image_rotated_180 = cv2.warpAffine(image, M_rot_180, (cols, rows))\n",
    "            image_rotated_270 = cv2.warpAffine(image, M_rot_270, (cols, rows))\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "            cv2.imwrite(PATH + \"/flip-xaxis-\" + file_name, image_xaxis_flipped)\n",
    "            cv2.imwrite(PATH + \"/flip-yaxis-\" + file_name, image_yaxis_flipped)\n",
    "            cv2.imwrite(PATH + \"/Edge Canny-\" + file_name + str(th1) + \"*\" + str(th2) + Extension, image_canny)\n",
    "            cv2.imwrite(PATH + \"/Translation-\" + file_name + str(x) + str(y) + Extension, image_translated)\n",
    "            cv2.imwrite(PATH + \"/Rotate-90-\" + file_name + str(deg) + Extension, image_rotated_90)\n",
    "            cv2.imwrite(PATH + \"/Rotate-180-\" + file_name + str(deg) + Extension, image_rotated_180)\n",
    "            cv2.imwrite(PATH + \"/Rotate-270-\" + file_name + str(deg) + Extension, image_rotated_270)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmentation(train_dir, Extension='.png', th1=100, th2=200, x=20, y=20, deg=90, deg2=180, deg3=270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5\n",
    "num_trees = 100\n",
    "\n",
    "def preprocess(images, labels):\n",
    "\n",
    "    dataDim = np.prod(images[0].shape)\n",
    "    images = np.array(images)\n",
    "    images = images.reshape(len(images), dataDim)\n",
    "    images = images.astype('float32')\n",
    "    images /=255\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "#function to preprocess data\n",
    "def preprocess_flatten(images, labels):\n",
    "\n",
    "    features = []\n",
    "    for image in images:\n",
    "        feature = np.reshape(image, (300*300))\n",
    "        features.append(feature)\n",
    "        \n",
    "    labels = np.array(labels)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def preprocess_canny_with_faltten(images, labels):\n",
    "    features = []\n",
    "    for i in range(len(images)):\n",
    "        \n",
    "        feature = canny((images[i]))\n",
    "        feature = np.reshape(feature, (300*300))\n",
    "        features.append(feature)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def preprocess_PCA_with_flatten(images, labels):\n",
    "    features = []\n",
    "    for i in range(len(images)):\n",
    "\n",
    "        image_pca = PCA().fit_transform(images[i])\n",
    "        image_selected = image_pca[:,:2]\n",
    "        image_selected = np.reshape(image_selected, (600))\n",
    "        features.append(image_selected)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def preprocess_canny_and_PCA_with_flatten(images, labels):\n",
    "\n",
    "    features = []\n",
    "    for i in range(len(images)):\n",
    "        \n",
    "        edges = canny((images[i]))\n",
    "        image_pca = PCA().fit_transform(edges)\n",
    "        image_selected = image_pca[:,:2]\n",
    "        image_selected = np.reshape(image_selected, (600))\n",
    "        features.append(image_selected)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    return features, labels\n",
    "\n",
    "# function to make classifier\n",
    "def classify(model, images, labels):\n",
    "\n",
    "    model.fit(images, labels)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = []\n",
    "val_accuracies = []\n",
    "models = []\n",
    "trained_models = []\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    models.append(('KNN', KNeighborsClassifier(n_neighbors=2)))\n",
    "    models.append(('LR', LogisticRegression(random_state=seed, max_iter=1000)))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('CART', DecisionTreeClassifier(random_state=seed)))\n",
    "    models.append(('RF', RandomForestClassifier(n_estimators=num_trees, random_state=seed)))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', SVC(random_state=seed)))\n",
    "    \n",
    "    #iterate through each shape\n",
    "    all_labels, all_images = [],[]\n",
    "    for shape in shape_list:\n",
    "        print('Getting data for: ', shape)\n",
    "        for file_name in os.listdir(os.path.join(train_dir,shape)):\n",
    "            all_images.append(cv2.imread(os.path.join(train_dir,shape,file_name), 0))\n",
    "            #add an integer to the labels list\n",
    "            all_labels.append(shape_list.index(shape))\n",
    "\n",
    "    # train and validation split\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(all_images, all_labels, \n",
    "                                                                          shuffle=True, stratify=all_labels, \n",
    "                                                                          test_size=0.1, random_state=42)\n",
    "\n",
    "    print('Number of training images: ', len(train_images), '\\n')\n",
    "\n",
    "    # Preprocess (your own function)\n",
    "    train_images, train_labels = preprocess(train_images, train_labels)\n",
    "    val_images, val_labels = preprocess(val_images, val_labels)\n",
    "\n",
    "    for name, model in models:\n",
    "        \n",
    "        best_model = model\n",
    "        #train_images, train_labels = preprocess(train_images, train_labels)\n",
    "        print (name)\n",
    "        \n",
    "        # Make a classifier (your own function)\n",
    "        model = classify(model, train_images, train_labels)\n",
    "        trained_models.append(model)\n",
    "\n",
    "        # Predict the labels from the model (your own code depending the output of the train function)\n",
    "        pred_labels = model.predict(train_images)\n",
    "\n",
    "        # Calculate accuracy (Do not erase or modify here)\n",
    "        pred_acc = np.sum(pred_labels==train_labels)/len(train_labels)*100\n",
    "        print(\"Accuracy = {}\".format(pred_acc))\n",
    "\n",
    "        cm = metrics.confusion_matrix(train_labels, pred_labels)\n",
    "        print(cm, '\\n')\n",
    "        \n",
    "        # Validation\n",
    "        print('Number of validation images: ', len(val_images))\n",
    "    \n",
    "        pred_val_labels = model.predict(val_images)\n",
    "        val_acc = np.sum(pred_val_labels==val_labels)/len(val_labels)*100\n",
    "        print(\"Val Accuracy = {}\".format(val_acc), '\\n')\n",
    "        \n",
    "        val_accuracies.append(val_acc)\n",
    "        names.append(name)\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for:  circle\n",
      "Getting data for:  triangle\n",
      "Getting data for:  tetragon\n",
      "Getting data for:  pentagon\n",
      "Getting data for:  other\n",
      "Number of training images:  2262 \n",
      "\n",
      "KNN\n",
      "Accuracy = 96.15384615384616\n",
      "[[451   0   0   0   0]\n",
      " [ 20 432   0   0   0]\n",
      " [  6   8 439   0   0]\n",
      " [  9   2   3 440   0]\n",
      " [ 26  10   2   1 413]] \n",
      "\n",
      "Number of validation images:  252\n",
      "Val Accuracy = 90.07936507936508 \n",
      "\n",
      "LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trojan/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0\n",
      "[[451   0   0   0   0]\n",
      " [  0 452   0   0   0]\n",
      " [  0   0 453   0   0]\n",
      " [  0   0   0 454   0]\n",
      " [  0   0   0   0 452]] \n",
      "\n",
      "Number of validation images:  252\n",
      "Val Accuracy = 47.22222222222222 \n",
      "\n",
      "LDA\n",
      "Accuracy = 88.01945181255526\n",
      "[[355  27  29   3  37]\n",
      " [ 30 404  12   0   6]\n",
      " [ 23  14 402   1  13]\n",
      " [ 20   6   6 420   2]\n",
      " [ 17  10  14   1 410]] \n",
      "\n",
      "Number of validation images:  252\n",
      "Val Accuracy = 45.63492063492063 \n",
      "\n",
      "CART\n",
      "Accuracy = 100.0\n",
      "[[451   0   0   0   0]\n",
      " [  0 452   0   0   0]\n",
      " [  0   0 453   0   0]\n",
      " [  0   0   0 454   0]\n",
      " [  0   0   0   0 452]] \n",
      "\n",
      "Number of validation images:  252\n",
      "Val Accuracy = 67.06349206349206 \n",
      "\n",
      "RF\n",
      "Accuracy = 100.0\n",
      "[[451   0   0   0   0]\n",
      " [  0 452   0   0   0]\n",
      " [  0   0 453   0   0]\n",
      " [  0   0   0 454   0]\n",
      " [  0   0   0   0 452]] \n",
      "\n",
      "Number of validation images:  252\n",
      "Val Accuracy = 85.31746031746032 \n",
      "\n",
      "NB\n",
      "Accuracy = 37.97524314765694\n",
      "[[438   1   1   2   9]\n",
      " [200  86   1   9 156]\n",
      " [149  51  65  51 137]\n",
      " [149  42   7 192  64]\n",
      " [362   8   1   3  78]] \n",
      "\n",
      "Number of validation images:  252\n",
      "Val Accuracy = 33.730158730158735 \n",
      "\n",
      "SVM\n",
      "Accuracy = 87.35632183908046\n",
      "[[419   6  14   0  12]\n",
      " [ 30 362  40   0  20]\n",
      " [  9  12 418   0  14]\n",
      " [ 56  30   0 367   1]\n",
      " [ 28   6   8   0 410]] \n",
      "\n",
      "Number of validation images:  252\n",
      "Val Accuracy = 77.77777777777779 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Accuracy = 90.07936507936508 \n",
      "\n",
      "('KNN', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
      "                     weights='uniform'))\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
      "                     weights='uniform')\n",
      "Best Model Is KNN \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose the best model\n",
    "\n",
    "best_acc = np.max(val_accuracies)\n",
    "print(\"Best Validation Accuracy = {}\".format(best_acc), '\\n')\n",
    "    \n",
    "index_best_acc = np.argmax(val_accuracies, axis=None)\n",
    "\n",
    "best_model_initial = models[index_best_acc]\n",
    "print(best_model_initial)\n",
    "\n",
    "best_model = trained_models[index_best_acc]\n",
    "print(best_model)\n",
    "\n",
    "best_model_name = names[index_best_acc]   \n",
    "print(\"Best Model Is {}\".format(best_model_name), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = \"final_model\"\n",
    "model_path = os.path.join(save_dir,filename)\n",
    "pickle.dump(best_model, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
      "                     weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "'''Model is provided in submission with report and notebook'''\n",
    "\n",
    "loaded_model = pickle.load(open(model_path, 'rb'))\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"forTA (Do not erase here)\n",
    "    test_dir = '../ForTA'\n",
    "    test_labels, test_images = [], []\n",
    "    for shape in shape_list:\n",
    "        print('Getting data for: ', shape)\n",
    "        for file_name in os.listdir(os.path.join(test_dir,shape)):\n",
    "            test_images.append(cv2.imread(os.path.join(test_dir,shape,file_name), 0))\n",
    "            #add an integer to the labels list\n",
    "            test_labels.append(shape_list.index(shape))\n",
    "\n",
    "    print('Number of test images: ', len(test_images))\n",
    "\n",
    "    test_images, test_labels = preprocess(test_images, test_labels)\n",
    "    #pred_labels = model.predict(test_images)\n",
    "    pred_labels = loaded_model.predict(test_images)\n",
    "    pred_acc = np.sum(pred_labels==test_labels)/len(test_labels)*100\n",
    "    print(\"Test Accuracy = {}\".format(pred_acc))\n",
    "    \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
