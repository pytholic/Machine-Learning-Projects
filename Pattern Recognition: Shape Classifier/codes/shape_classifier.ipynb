{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists to save the labels (the name of the shape)\n",
    "train_dir = '/home/trojan/Desktop/pattern recognition/PB1/Implementation/data/shapes'\n",
    "shape_list = ['circle', 'triangle', 'tetragon', 'pentagon', 'other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5\n",
    "num_trees = 100\n",
    "\n",
    "#function to preprocess data\n",
    "def preprocess(images, labels):\n",
    "    \"\"\"You can make your preprocessing code in this function.\n",
    "    Here, we just flatten the images, for example.\n",
    "    In addition, you can split this data into the training set and validation set for robustness to the test(unseen) data.\n",
    "\n",
    "    :params list images: (Number of images x row x column)\n",
    "    :params list labels: (Number of images, 1)\n",
    "    :rtype: array\n",
    "    :return: preprocessed images and labels\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    dataDim = np.prod(images[0].shape)\n",
    "    images = np.array(images)\n",
    "    images = images.reshape(len(images), dataDim)\n",
    "    images = images.astype('float32')\n",
    "    images /=255\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# function to make classifier\n",
    "def classify(model, images, labels):\n",
    "    \"\"\"You can make your classifier code in this function.\n",
    "    Here, we use KNN classifier, for example.\n",
    "\n",
    "    :params array images: (Number of images x row x column)\n",
    "    :params array labels: (Number of images)\n",
    "    :return: classifier model\n",
    "    \"\"\"\n",
    "    model.fit(images, labels)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    models = []\n",
    "    names = []\n",
    "    models.append(('KNN', KNeighborsClassifier(n_neighbors=2)))\n",
    "    models.append(('LR', LogisticRegression(random_state=seed)))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('CART', DecisionTreeClassifier(random_state=seed)))\n",
    "    models.append(('RF', RandomForestClassifier(n_estimators=num_trees, random_state=seed)))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', SVC(random_state=seed)))\n",
    "    \n",
    "    #iterate through each shape\n",
    "    all_labels, all_images = [],[]\n",
    "    for shape in shape_list:\n",
    "        print('Getting data for: ', shape)\n",
    "        for file_name in os.listdir(os.path.join(train_dir,shape)):\n",
    "            all_images.append(cv2.imread(os.path.join(train_dir,shape,file_name), 0))\n",
    "            #add an integer to the labels list\n",
    "            all_labels.append(shape_list.index(shape))\n",
    "\n",
    "    # train and validation split\n",
    "    train_images, val_images, train_labels, val_labels = train_test_split(all_images, all_labels, \n",
    "                                                                          shuffle=True, stratify=all_labels, \n",
    "                                                                          test_size=0.4, random_state=42)\n",
    "\n",
    "    print('Number of training images: ', len(train_images), '\\n')\n",
    "\n",
    "    # Preprocess (your own function)\n",
    "    train_images, train_labels = preprocess(train_images, train_labels)\n",
    "\n",
    "    for name, model in models:\n",
    "        \n",
    "        print (name)\n",
    "        \n",
    "        # Make a classifier (your own function)\n",
    "        model = classify(model, train_images, train_labels)\n",
    "\n",
    "        # Predict the labels from the model (your own code depending the output of the train function)\n",
    "        pred_labels = model.predict(train_images)\n",
    "\n",
    "        # Calculate accuracy (Do not erase or modify here)\n",
    "        pred_acc = np.sum(pred_labels==train_labels)/len(train_labels)*100\n",
    "        print(\"Accuracy = {}\".format(pred_acc))\n",
    "\n",
    "        cm = metrics.confusion_matrix(train_labels, pred_labels)\n",
    "        print(cm, '\\n')\n",
    "        \n",
    "        # Validation\n",
    "        print('Number of validation images: ', len(val_images))\n",
    "    \n",
    "        val_images, val_labels = preprocess(val_images, val_labels)\n",
    "    \n",
    "        pred_val_labels = model.predict(val_images)\n",
    "        val_acc = np.sum(pred_val_labels==val_labels)/len(val_labels)*100\n",
    "        print(\"Val Accuracy = {}\".format(val_acc), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for:  circle\n",
      "Getting data for:  triangle\n",
      "Getting data for:  tetragon\n",
      "Getting data for:  pentagon\n",
      "Getting data for:  other\n",
      "Number of training images:  15 \n",
      "\n",
      "KNN\n",
      "Accuracy = 60.0\n",
      "[[3 0 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [1 0 2 0 0]\n",
      " [1 0 0 2 0]\n",
      " [1 0 1 0 1]] \n",
      "\n",
      "Number of validation images:  10\n",
      "Val Accuracy = 20.0 \n",
      "\n",
      "LR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trojan/.local/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 100.0\n",
      "[[3 0 0 0 0]\n",
      " [0 3 0 0 0]\n",
      " [0 0 3 0 0]\n",
      " [0 0 0 3 0]\n",
      " [0 0 0 0 3]] \n",
      "\n",
      "Number of validation images:  10\n",
      "Val Accuracy = 20.0 \n",
      "\n",
      "LDA\n",
      "Accuracy = 53.333333333333336\n",
      "[[1 1 0 0 1]\n",
      " [0 2 1 0 0]\n",
      " [1 0 1 0 1]\n",
      " [1 0 0 2 0]\n",
      " [0 0 1 0 2]] \n",
      "\n",
      "Number of validation images:  10\n",
      "Val Accuracy = 20.0 \n",
      "\n",
      "CART\n",
      "Accuracy = 100.0\n",
      "[[3 0 0 0 0]\n",
      " [0 3 0 0 0]\n",
      " [0 0 3 0 0]\n",
      " [0 0 0 3 0]\n",
      " [0 0 0 0 3]] \n",
      "\n",
      "Number of validation images:  10\n",
      "Val Accuracy = 20.0 \n",
      "\n",
      "RF\n",
      "Accuracy = 100.0\n",
      "[[3 0 0 0 0]\n",
      " [0 3 0 0 0]\n",
      " [0 0 3 0 0]\n",
      " [0 0 0 3 0]\n",
      " [0 0 0 0 3]] \n",
      "\n",
      "Number of validation images:  10\n",
      "Val Accuracy = 20.0 \n",
      "\n",
      "NB\n",
      "Accuracy = 100.0\n",
      "[[3 0 0 0 0]\n",
      " [0 3 0 0 0]\n",
      " [0 0 3 0 0]\n",
      " [0 0 0 3 0]\n",
      " [0 0 0 0 3]] \n",
      "\n",
      "Number of validation images:  10\n",
      "Val Accuracy = 20.0 \n",
      "\n",
      "SVM\n",
      "Accuracy = 93.33333333333333\n",
      "[[3 0 0 0 0]\n",
      " [0 3 0 0 0]\n",
      " [0 0 3 0 0]\n",
      " [1 0 0 2 0]\n",
      " [0 0 0 0 3]] \n",
      "\n",
      "Number of validation images:  10\n",
      "Val Accuracy = 20.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forTA (Do not erase here)\\ntest_dir = \\'../ForTA\\'\\ntest_labels, test_images = [], []\\nfor shape in shape_list:\\n    print(\\'Getting data for: \\', shape)\\n    for file_name in os.listdir(os.path.join(test_dir,shape)):\\n        test_images.append(cv2.imread(os.path.join(test_dir,shape,file_name), 0))\\n        #add an integer to the labels list\\n        test_labels.append(shape_list.index(shape))\\n\\nprint(\\'Number of test images: \\', len(test_images))\\n\\ntest_images, test_labels = preprocess(test_images, test_labels)\\npred_labels = model.predict(test_images)\\npred_acc = np.sum(pred_labels==test_labels)/len(test_labels)*100\\nprint(\"Test Accuracy = {}\".format(pred_acc))\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \"\"\"forTA (Do not erase here)\n",
    "    test_dir = '../ForTA'\n",
    "    test_labels, test_images = [], []\n",
    "    for shape in shape_list:\n",
    "        print('Getting data for: ', shape)\n",
    "        for file_name in os.listdir(os.path.join(test_dir,shape)):\n",
    "            test_images.append(cv2.imread(os.path.join(test_dir,shape,file_name), 0))\n",
    "            #add an integer to the labels list\n",
    "            test_labels.append(shape_list.index(shape))\n",
    "\n",
    "    print('Number of test images: ', len(test_images))\n",
    "\n",
    "    test_images, test_labels = preprocess(test_images, test_labels)\n",
    "    pred_labels = model.predict(test_images)\n",
    "    pred_acc = np.sum(pred_labels==test_labels)/len(test_labels)*100\n",
    "    print(\"Test Accuracy = {}\".format(pred_acc))\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
