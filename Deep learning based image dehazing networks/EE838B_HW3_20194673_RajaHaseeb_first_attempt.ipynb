{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Main Code'''\n",
    "\n",
    "#Import libraries and scripts\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import io\n",
    "\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from math import log10\n",
    "from torchvision.utils import save_image as imwrite\n",
    "\n",
    "from model import ConvNetSep\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "import data\n",
    "from data import triplet_train, triplet_test\n",
    "\n",
    "import model\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if gpu is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental setup and argument parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using easydict instead of argparser because I am using notebook\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "args = edict()\n",
    "\n",
    "# Training data\n",
    "args.data_dir = '/home/trojan/Desktop/Image restoration/Homeworks/HW3/dataset/Dataet_VFI_HW3/Vimeo90K_HW3'  # training data \n",
    "args.save_dir = '/home/trojan/Desktop/Image restoration/Homeworks/HW3/result'   # results directory\n",
    "\n",
    "# Model\n",
    "args.exp_name = 'Net_SepConv'   # model to be selected\n",
    "args.finetuning = False   # to finetune the training\n",
    "args.load = None #'NetFinal'\n",
    "\n",
    "#Validation data\n",
    "args.val_data = True\n",
    "args.val_batch_size = 1   # batch size for validation data\n",
    "#args.n_threads = 8   # threads number for loading data'''\n",
    "\n",
    "# Testing data\n",
    "args.test_dir = '/home/trojan/Desktop/Image restoration/Homeworks/HW3/dataset/Dataet_VFI_HW3/ucf101_HW3'   # test dataset directory\n",
    "args.save = True\n",
    "\n",
    "# Training and Optimization\n",
    "args.patch_size = 128\n",
    "args.batch_size = 8\n",
    "args.kernel_size = 25\n",
    "args.lr = 1e-4   # learning rate for the optimizer\n",
    "args.epochs = 200   # number of training epochs\n",
    "#args.lr_step_size = 600   # decay learning rate after N epochs\n",
    "#args.lr_gamma = 0.1   # learning rate decay factor\n",
    "args.lr_decay = 100   #number of epochs to drop lr\n",
    "args.decay_type = 'step' #lr decay type\n",
    "args.loss_type = 'L1'   #Loss type\n",
    "\n",
    "args.period = 5\n",
    "args.gpu = True   # gpu index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Settings\n",
    "if args.gpu == 0:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "elif args.gpu == 1:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Check cuda device\n",
    "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "def to_variable(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def set_loss(args):\n",
    "    loss_type = args.loss_type\n",
    "    if loss_type == 'MSE':\n",
    "        lossfunction = nn.MSELoss()\n",
    "    elif loss_type == 'L1':\n",
    "        lossfunction = nn.L1Loss()\n",
    "    return lossfunction\n",
    "\n",
    "def set_lr(args, epoch, optimizer):\n",
    "    lr_decay = args.lr_decay\n",
    "    decay_type = args.decay_type\n",
    "    if decay_type == 'step':\n",
    "        epoch_iter = (epoch + 1) // lr_decay\n",
    "        lr = args.lr / 2 ** epoch_iter\n",
    "    elif decay_type == 'exp':\n",
    "        k = math.log(2) / lr_decay\n",
    "        lr = args.lr * math.exp(-k * epoch)\n",
    "    elif decay_type == 'inv':\n",
    "        k = 1 / lr_decay\n",
    "        lr = args.lr / (1 + k * epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def inc_epoch(self):\n",
    "    self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, Dataset, save_dir=args.save_dir, output_name='output.png'):\n",
    "        test_dir = args.test_dir\n",
    "        patch_size = args.patch_size\n",
    "         \n",
    "        avg_psnr = 0    \n",
    "        test_loader = DataLoader(dataset=Dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "        total_batches = 0 \n",
    "        for batch_index, (Frame1, Frame2, Frame3) in enumerate(test_loader):\n",
    "            Frame1 = to_variable(Frame1)\n",
    "            Frame2 = to_variable(Frame2)\n",
    "            Frame3 = to_variable(Frame3)\n",
    "            frame_out = model(Frame1, Frame3)\n",
    "            gt = Frame2\n",
    "            psnr = -10 * log10(torch.mean((gt - frame_out) * (gt - frame_out)).item())\n",
    "            avg_psnr += psnr\n",
    "            imwrite(frame_out, save_dir + '/' + '{}'.format(batch_index) + output_name, range=(0, 1))\n",
    "            \n",
    "            msg = \"Batch: {}\\t PSNR: {:<20.4f}\\n\".format(batch_index, psnr)\n",
    "            total_batches += 1\n",
    "            #print(msg, end='')\n",
    "            #logfile.write(msg)\n",
    "            \n",
    "        avg_psnr /= total_batches\n",
    "        return avg_psnr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \n",
    "    train_dir = args.data_dir\n",
    "    save_dir = args.save_dir\n",
    "    test_dir = args.test_dir\n",
    "    batch_size = args.batch_size\n",
    "    total_epochs = args.epochs\n",
    "    patch_size = args.patch_size\n",
    "    \n",
    "    loss_function = set_loss(args)\n",
    "    loss_function.cuda()\n",
    "        \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    ckptDir = save_dir + '/chekpoint'\n",
    "    resultDir = save_dir \n",
    "    \n",
    "    if not os.path.exists(ckptDir):\n",
    "        os.makedirs(ckptDir)\n",
    "        \n",
    "    dataset = triplet_train(train_dir, resize=(patch_size, patch_size))\n",
    "    train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    test_dataset = triplet_test(test_dir, resize=None)\n",
    "    test_resultDir = save_dir + '/test_result'\n",
    "    \n",
    "    test_output_dir = test_resultDir\n",
    "    \n",
    "    if not os.path.exists(test_resultDir):\n",
    "        os.makedirs(test_resultDir)\n",
    "    \n",
    "    log_dir = save_dir + '/logging'\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        \n",
    "    if os.path.exists(log_dir + '/log.txt'):\n",
    "        logFile = open(log_dir + '/log.txt', 'a')\n",
    "    else:\n",
    "        logFile = open(log_dir + '/log.txt', 'w')\n",
    "    \n",
    "    logFile.write('Batch size: ' + str(batch_size) + '\\n')\n",
    "    \n",
    "    if args.load is not None:\n",
    "        checkpoint = torch.load(args.load)\n",
    "        kernel_size = checkpoint['kernel_size']\n",
    "        model = ConvNetSep(kernel_size=kernel_size)\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        kernel_size = args.kernel_size\n",
    "        model = ConvNetSep(kernel_size=kernel_size)\n",
    "\n",
    "    logFile.write('Kernel size: ' + str(kernel_size) + '\\n')\n",
    "    num_params = count_parameters(model)\n",
    "    logFile.write('Parameters: ' + str(num_params) + '\\n')\n",
    "    \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    start_epoch = 0\n",
    "    epoch_time = 0\n",
    "    total_time = 0\n",
    "    max_step = train_loader.__len__()\n",
    "    \n",
    "    #model.eval()\n",
    "    #test(model, test_dataset, start_epoch, test_output_dir, logfile, output_name = 'output.png')\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "        start = time.time()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "        learning_rate = set_lr(args, epoch, optimizer)\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, total_epochs))\n",
    "        \n",
    "        model.train()\n",
    "        for batch_index, (Frame1, Frame2, Frame3) in enumerate(train_loader):\n",
    "            Frame1 = to_variable(Frame1)\n",
    "            Frame2 = to_variable(Frame2)\n",
    "            Frame3 = to_variable(Frame3)\n",
    "            output = model(Frame1, Frame3)\n",
    "            loss = loss_function(output, Frame2)\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "            \n",
    "        end = time.time()\n",
    "        epoch_time = (end - start)\n",
    "        total_time += epoch_time\n",
    "        #model.inc_epoch()\n",
    "        \n",
    "        #if batch_index % 100 == 0:\n",
    "        #if  (epoch + 1) % args.period == 0:\n",
    "        log = \"Epoch {}/{} \\t Learning rate: {:.5f} \\t Train Loss: {:.5f} \\t Epoch Time: {:.4f} \\t Total time: {:.4f}\\n\".format(epoch + 1, total_epochs,\n",
    "                                                                                                                             learning_rate, loss, epoch_time, total_time)\n",
    "        print(log)\n",
    "        logFile.write(log)\n",
    "                \n",
    "        if  (epoch + 1) % args.period == 0:\n",
    "                \n",
    "            if args.val_data:\n",
    "                torch.save({'epoch': model.epoch, 'state_dict': model.state_dict(), 'kernel_size': kernel_size}, ckptDir + '/model_epoch' + str(model.epoch).zfill(3) + '.pth')\n",
    "                model.eval()\n",
    "                avg_psnr = test(model, test_dataset, epoch, test_output_dir, output_name = 'output.png')\n",
    "                msg = 'Average PSNR: {:<20.4f}'.format(avg_psnr) + '\\n'\n",
    "                print(msg)\n",
    "                logFile.write(msg)\n",
    "                logFile.write('\\n\\n')\n",
    "        logFile.flush()\n",
    "        \n",
    "    logFile.close()\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Epoch 1/200 \t Learning rate: 0.00010 \t Train Loss: 0.03167 t Epoch Time: 76.3662 \t Total time: 76.3662\n",
      "\n",
      "Epoch 2/200\n",
      "Epoch 2/200 \t Learning rate: 0.00010 \t Train Loss: 0.02818 t Epoch Time: 79.0506 \t Total time: 155.4168\n",
      "\n",
      "Epoch 3/200\n",
      "Epoch 3/200 \t Learning rate: 0.00010 \t Train Loss: 0.02585 t Epoch Time: 79.9360 \t Total time: 235.3527\n",
      "\n",
      "Epoch 4/200\n",
      "Epoch 4/200 \t Learning rate: 0.00010 \t Train Loss: 0.02670 t Epoch Time: 79.2658 \t Total time: 314.6185\n",
      "\n",
      "Epoch 5/200\n",
      "Epoch 5/200 \t Learning rate: 0.00010 \t Train Loss: 0.02995 t Epoch Time: 79.7091 \t Total time: 394.3276\n",
      "\n",
      "Epoch 6/200\n",
      "Epoch 6/200 \t Learning rate: 0.00010 \t Train Loss: 0.02044 t Epoch Time: 79.1878 \t Total time: 473.5155\n",
      "\n",
      "Epoch 7/200\n",
      "Epoch 7/200 \t Learning rate: 0.00010 \t Train Loss: 0.01788 t Epoch Time: 74.6735 \t Total time: 548.1890\n",
      "\n",
      "Epoch 8/200\n",
      "Epoch 8/200 \t Learning rate: 0.00010 \t Train Loss: 0.02061 t Epoch Time: 74.6018 \t Total time: 622.7908\n",
      "\n",
      "Epoch 9/200\n",
      "Epoch 9/200 \t Learning rate: 0.00010 \t Train Loss: 0.01441 t Epoch Time: 72.4152 \t Total time: 695.2060\n",
      "\n",
      "Epoch 10/200\n",
      "Epoch 10/200 \t Learning rate: 0.00010 \t Train Loss: 0.01093 t Epoch Time: 71.9773 \t Total time: 767.1833\n",
      "\n",
      "Epoch 11/200\n",
      "Epoch 11/200 \t Learning rate: 0.00010 \t Train Loss: 0.01532 t Epoch Time: 72.0312 \t Total time: 839.2144\n",
      "\n",
      "Epoch 12/200\n",
      "Epoch 12/200 \t Learning rate: 0.00010 \t Train Loss: 0.01407 t Epoch Time: 72.0308 \t Total time: 911.2452\n",
      "\n",
      "Epoch 13/200\n",
      "Epoch 13/200 \t Learning rate: 0.00010 \t Train Loss: 0.01988 t Epoch Time: 71.8891 \t Total time: 983.1343\n",
      "\n",
      "Epoch 14/200\n",
      "Epoch 14/200 \t Learning rate: 0.00010 \t Train Loss: 0.01747 t Epoch Time: 71.8722 \t Total time: 1055.0065\n",
      "\n",
      "Epoch 15/200\n",
      "Epoch 15/200 \t Learning rate: 0.00010 \t Train Loss: 0.02221 t Epoch Time: 71.9560 \t Total time: 1126.9625\n",
      "\n",
      "Epoch 16/200\n",
      "Epoch 16/200 \t Learning rate: 0.00010 \t Train Loss: 0.02418 t Epoch Time: 74.6116 \t Total time: 1201.5741\n",
      "\n",
      "Epoch 17/200\n",
      "Epoch 17/200 \t Learning rate: 0.00010 \t Train Loss: 0.01282 t Epoch Time: 71.6526 \t Total time: 1273.2267\n",
      "\n",
      "Epoch 18/200\n",
      "Epoch 18/200 \t Learning rate: 0.00010 \t Train Loss: 0.01600 t Epoch Time: 71.7327 \t Total time: 1344.9594\n",
      "\n",
      "Epoch 19/200\n",
      "Epoch 19/200 \t Learning rate: 0.00010 \t Train Loss: 0.02543 t Epoch Time: 71.7704 \t Total time: 1416.7298\n",
      "\n",
      "Epoch 20/200\n",
      "Epoch 20/200 \t Learning rate: 0.00010 \t Train Loss: 0.01922 t Epoch Time: 71.7919 \t Total time: 1488.5217\n",
      "\n",
      "Epoch 21/200\n",
      "Epoch 21/200 \t Learning rate: 0.00010 \t Train Loss: 0.01252 t Epoch Time: 74.0580 \t Total time: 1562.5798\n",
      "\n",
      "Epoch 22/200\n",
      "Epoch 22/200 \t Learning rate: 0.00010 \t Train Loss: 0.01833 t Epoch Time: 72.0631 \t Total time: 1634.6429\n",
      "\n",
      "Epoch 23/200\n",
      "Epoch 23/200 \t Learning rate: 0.00010 \t Train Loss: 0.01667 t Epoch Time: 71.7217 \t Total time: 1706.3646\n",
      "\n",
      "Epoch 24/200\n",
      "Epoch 24/200 \t Learning rate: 0.00010 \t Train Loss: 0.01162 t Epoch Time: 71.7293 \t Total time: 1778.0939\n",
      "\n",
      "Epoch 25/200\n",
      "Epoch 25/200 \t Learning rate: 0.00010 \t Train Loss: 0.01159 t Epoch Time: 71.3474 \t Total time: 1849.4413\n",
      "\n",
      "Epoch 26/200\n",
      "Epoch 26/200 \t Learning rate: 0.00010 \t Train Loss: 0.01245 t Epoch Time: 71.1855 \t Total time: 1920.6267\n",
      "\n",
      "Epoch 27/200\n",
      "Epoch 27/200 \t Learning rate: 0.00010 \t Train Loss: 0.01392 t Epoch Time: 71.3100 \t Total time: 1991.9368\n",
      "\n",
      "Epoch 28/200\n",
      "Epoch 28/200 \t Learning rate: 0.00010 \t Train Loss: 0.01317 t Epoch Time: 71.3529 \t Total time: 2063.2897\n",
      "\n",
      "Epoch 29/200\n",
      "Epoch 29/200 \t Learning rate: 0.00010 \t Train Loss: 0.01560 t Epoch Time: 72.7337 \t Total time: 2136.0234\n",
      "\n",
      "Epoch 30/200\n",
      "Epoch 30/200 \t Learning rate: 0.00010 \t Train Loss: 0.01585 t Epoch Time: 73.7877 \t Total time: 2209.8111\n",
      "\n",
      "Epoch 31/200\n",
      "Epoch 31/200 \t Learning rate: 0.00010 \t Train Loss: 0.02315 t Epoch Time: 73.7129 \t Total time: 2283.5240\n",
      "\n",
      "Epoch 32/200\n",
      "Epoch 32/200 \t Learning rate: 0.00010 \t Train Loss: 0.00778 t Epoch Time: 73.5643 \t Total time: 2357.0883\n",
      "\n",
      "Epoch 33/200\n",
      "Epoch 33/200 \t Learning rate: 0.00010 \t Train Loss: 0.01483 t Epoch Time: 73.4970 \t Total time: 2430.5852\n",
      "\n",
      "Epoch 34/200\n",
      "Epoch 34/200 \t Learning rate: 0.00010 \t Train Loss: 0.01017 t Epoch Time: 73.5345 \t Total time: 2504.1197\n",
      "\n",
      "Epoch 35/200\n",
      "Epoch 35/200 \t Learning rate: 0.00010 \t Train Loss: 0.00862 t Epoch Time: 74.4916 \t Total time: 2578.6113\n",
      "\n",
      "Epoch 36/200\n",
      "Epoch 36/200 \t Learning rate: 0.00010 \t Train Loss: 0.01248 t Epoch Time: 74.8131 \t Total time: 2653.4243\n",
      "\n",
      "Epoch 37/200\n",
      "Epoch 37/200 \t Learning rate: 0.00010 \t Train Loss: 0.01437 t Epoch Time: 74.0458 \t Total time: 2727.4701\n",
      "\n",
      "Epoch 38/200\n",
      "Epoch 38/200 \t Learning rate: 0.00010 \t Train Loss: 0.01288 t Epoch Time: 73.8930 \t Total time: 2801.3632\n",
      "\n",
      "Epoch 39/200\n",
      "Epoch 39/200 \t Learning rate: 0.00010 \t Train Loss: 0.00815 t Epoch Time: 73.8654 \t Total time: 2875.2286\n",
      "\n",
      "Epoch 40/200\n",
      "Epoch 40/200 \t Learning rate: 0.00010 \t Train Loss: 0.01101 t Epoch Time: 73.8817 \t Total time: 2949.1103\n",
      "\n",
      "Epoch 41/200\n",
      "Epoch 41/200 \t Learning rate: 0.00010 \t Train Loss: 0.01713 t Epoch Time: 71.3409 \t Total time: 3020.4512\n",
      "\n",
      "Epoch 42/200\n",
      "Epoch 42/200 \t Learning rate: 0.00010 \t Train Loss: 0.01546 t Epoch Time: 71.9372 \t Total time: 3092.3885\n",
      "\n",
      "Epoch 43/200\n",
      "Epoch 43/200 \t Learning rate: 0.00010 \t Train Loss: 0.01726 t Epoch Time: 75.3185 \t Total time: 3167.7070\n",
      "\n",
      "Epoch 44/200\n",
      "Epoch 44/200 \t Learning rate: 0.00010 \t Train Loss: 0.01185 t Epoch Time: 75.5299 \t Total time: 3243.2369\n",
      "\n",
      "Epoch 45/200\n",
      "Epoch 45/200 \t Learning rate: 0.00010 \t Train Loss: 0.00947 t Epoch Time: 75.3205 \t Total time: 3318.5573\n",
      "\n",
      "Epoch 46/200\n",
      "Epoch 46/200 \t Learning rate: 0.00010 \t Train Loss: 0.00839 t Epoch Time: 72.5468 \t Total time: 3391.1042\n",
      "\n",
      "Epoch 47/200\n",
      "Epoch 47/200 \t Learning rate: 0.00010 \t Train Loss: 0.00884 t Epoch Time: 72.7123 \t Total time: 3463.8164\n",
      "\n",
      "Epoch 48/200\n",
      "Epoch 48/200 \t Learning rate: 0.00010 \t Train Loss: 0.01301 t Epoch Time: 72.7530 \t Total time: 3536.5695\n",
      "\n",
      "Epoch 49/200\n",
      "Epoch 49/200 \t Learning rate: 0.00010 \t Train Loss: 0.01051 t Epoch Time: 73.3991 \t Total time: 3609.9686\n",
      "\n",
      "Epoch 50/200\n",
      "Epoch 50/200 \t Learning rate: 0.00010 \t Train Loss: 0.01084 t Epoch Time: 74.2729 \t Total time: 3684.2415\n",
      "\n",
      "Epoch 51/200\n",
      "Epoch 51/200 \t Learning rate: 0.00010 \t Train Loss: 0.00673 t Epoch Time: 73.7403 \t Total time: 3757.9817\n",
      "\n",
      "Epoch 52/200\n",
      "Epoch 52/200 \t Learning rate: 0.00010 \t Train Loss: 0.00831 t Epoch Time: 73.7124 \t Total time: 3831.6941\n",
      "\n",
      "Epoch 53/200\n",
      "Epoch 53/200 \t Learning rate: 0.00010 \t Train Loss: 0.01061 t Epoch Time: 73.7040 \t Total time: 3905.3981\n",
      "\n",
      "Epoch 54/200\n",
      "Epoch 54/200 \t Learning rate: 0.00010 \t Train Loss: 0.00779 t Epoch Time: 73.7869 \t Total time: 3979.1850\n",
      "\n",
      "Epoch 55/200\n",
      "Epoch 55/200 \t Learning rate: 0.00010 \t Train Loss: 0.00829 t Epoch Time: 73.6918 \t Total time: 4052.8768\n",
      "\n",
      "Epoch 56/200\n",
      "Epoch 56/200 \t Learning rate: 0.00010 \t Train Loss: 0.00650 t Epoch Time: 74.0179 \t Total time: 4126.8947\n",
      "\n",
      "Epoch 57/200\n",
      "Epoch 57/200 \t Learning rate: 0.00010 \t Train Loss: 0.01015 t Epoch Time: 73.7136 \t Total time: 4200.6082\n",
      "\n",
      "Epoch 58/200\n",
      "Epoch 58/200 \t Learning rate: 0.00010 \t Train Loss: 0.01109 t Epoch Time: 73.6855 \t Total time: 4274.2937\n",
      "\n",
      "Epoch 59/200\n",
      "Epoch 59/200 \t Learning rate: 0.00010 \t Train Loss: 0.00776 t Epoch Time: 73.7804 \t Total time: 4348.0741\n",
      "\n",
      "Epoch 60/200\n",
      "Epoch 60/200 \t Learning rate: 0.00010 \t Train Loss: 0.00962 t Epoch Time: 72.0877 \t Total time: 4420.1618\n",
      "\n",
      "Epoch 61/200\n",
      "Epoch 61/200 \t Learning rate: 0.00010 \t Train Loss: 0.00858 t Epoch Time: 71.3420 \t Total time: 4491.5038\n",
      "\n",
      "Epoch 62/200\n",
      "Epoch 62/200 \t Learning rate: 0.00010 \t Train Loss: 0.00779 t Epoch Time: 71.4261 \t Total time: 4562.9299\n",
      "\n",
      "Epoch 63/200\n",
      "Epoch 63/200 \t Learning rate: 0.00010 \t Train Loss: 0.01014 t Epoch Time: 71.4021 \t Total time: 4634.3319\n",
      "\n",
      "Epoch 64/200\n",
      "Epoch 64/200 \t Learning rate: 0.00010 \t Train Loss: 0.00842 t Epoch Time: 71.4781 \t Total time: 4705.8100\n",
      "\n",
      "Epoch 65/200\n",
      "Epoch 65/200 \t Learning rate: 0.00010 \t Train Loss: 0.00919 t Epoch Time: 71.6784 \t Total time: 4777.4884\n",
      "\n",
      "Epoch 66/200\n",
      "Epoch 66/200 \t Learning rate: 0.00010 \t Train Loss: 0.00845 t Epoch Time: 71.0422 \t Total time: 4848.5306\n",
      "\n",
      "Epoch 67/200\n",
      "Epoch 67/200 \t Learning rate: 0.00010 \t Train Loss: 0.01013 t Epoch Time: 74.4557 \t Total time: 4922.9863\n",
      "\n",
      "Epoch 68/200\n",
      "Epoch 68/200 \t Learning rate: 0.00010 \t Train Loss: 0.01246 t Epoch Time: 73.4349 \t Total time: 4996.4212\n",
      "\n",
      "Epoch 69/200\n",
      "Epoch 69/200 \t Learning rate: 0.00010 \t Train Loss: 0.00840 t Epoch Time: 73.4758 \t Total time: 5069.8970\n",
      "\n",
      "Epoch 70/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200 \t Learning rate: 0.00010 \t Train Loss: 0.01134 t Epoch Time: 75.8123 \t Total time: 5145.7093\n",
      "\n",
      "Epoch 71/200\n",
      "Epoch 71/200 \t Learning rate: 0.00010 \t Train Loss: 0.01846 t Epoch Time: 70.4316 \t Total time: 5216.1409\n",
      "\n",
      "Epoch 72/200\n",
      "Epoch 72/200 \t Learning rate: 0.00010 \t Train Loss: 0.01285 t Epoch Time: 70.8689 \t Total time: 5287.0098\n",
      "\n",
      "Epoch 73/200\n",
      "Epoch 73/200 \t Learning rate: 0.00010 \t Train Loss: 0.01273 t Epoch Time: 70.8892 \t Total time: 5357.8990\n",
      "\n",
      "Epoch 74/200\n",
      "Epoch 74/200 \t Learning rate: 0.00010 \t Train Loss: 0.00702 t Epoch Time: 71.2181 \t Total time: 5429.1171\n",
      "\n",
      "Epoch 75/200\n",
      "Epoch 75/200 \t Learning rate: 0.00010 \t Train Loss: 0.00683 t Epoch Time: 74.9293 \t Total time: 5504.0464\n",
      "\n",
      "Epoch 76/200\n",
      "Epoch 76/200 \t Learning rate: 0.00010 \t Train Loss: 0.00843 t Epoch Time: 71.8490 \t Total time: 5575.8954\n",
      "\n",
      "Epoch 77/200\n",
      "Epoch 77/200 \t Learning rate: 0.00010 \t Train Loss: 0.00757 t Epoch Time: 71.6904 \t Total time: 5647.5858\n",
      "\n",
      "Epoch 78/200\n",
      "Epoch 78/200 \t Learning rate: 0.00010 \t Train Loss: 0.01811 t Epoch Time: 71.3969 \t Total time: 5718.9827\n",
      "\n",
      "Epoch 79/200\n",
      "Epoch 79/200 \t Learning rate: 0.00010 \t Train Loss: 0.00842 t Epoch Time: 71.3535 \t Total time: 5790.3363\n",
      "\n",
      "Epoch 80/200\n",
      "Epoch 80/200 \t Learning rate: 0.00010 \t Train Loss: 0.00941 t Epoch Time: 71.3859 \t Total time: 5861.7222\n",
      "\n",
      "Epoch 81/200\n",
      "Epoch 81/200 \t Learning rate: 0.00010 \t Train Loss: 0.00751 t Epoch Time: 71.4726 \t Total time: 5933.1948\n",
      "\n",
      "Epoch 82/200\n",
      "Epoch 82/200 \t Learning rate: 0.00010 \t Train Loss: 0.00633 t Epoch Time: 72.0167 \t Total time: 6005.2115\n",
      "\n",
      "Epoch 83/200\n",
      "Epoch 83/200 \t Learning rate: 0.00010 \t Train Loss: 0.00528 t Epoch Time: 74.4059 \t Total time: 6079.6174\n",
      "\n",
      "Epoch 84/200\n",
      "Epoch 84/200 \t Learning rate: 0.00010 \t Train Loss: 0.00995 t Epoch Time: 74.4130 \t Total time: 6154.0303\n",
      "\n",
      "Epoch 85/200\n",
      "Epoch 85/200 \t Learning rate: 0.00010 \t Train Loss: 0.00748 t Epoch Time: 72.4851 \t Total time: 6226.5154\n",
      "\n",
      "Epoch 86/200\n",
      "Epoch 86/200 \t Learning rate: 0.00010 \t Train Loss: 0.01470 t Epoch Time: 72.8776 \t Total time: 6299.3929\n",
      "\n",
      "Epoch 87/200\n",
      "Epoch 87/200 \t Learning rate: 0.00010 \t Train Loss: 0.00932 t Epoch Time: 73.6246 \t Total time: 6373.0175\n",
      "\n",
      "Epoch 88/200\n",
      "Epoch 88/200 \t Learning rate: 0.00010 \t Train Loss: 0.00814 t Epoch Time: 73.2493 \t Total time: 6446.2668\n",
      "\n",
      "Epoch 89/200\n",
      "Epoch 89/200 \t Learning rate: 0.00010 \t Train Loss: 0.01755 t Epoch Time: 71.7199 \t Total time: 6517.9866\n",
      "\n",
      "Epoch 90/200\n",
      "Epoch 90/200 \t Learning rate: 0.00010 \t Train Loss: 0.01145 t Epoch Time: 74.9554 \t Total time: 6592.9421\n",
      "\n",
      "Epoch 91/200\n",
      "Epoch 91/200 \t Learning rate: 0.00010 \t Train Loss: 0.01291 t Epoch Time: 75.7842 \t Total time: 6668.7262\n",
      "\n",
      "Epoch 92/200\n",
      "Epoch 92/200 \t Learning rate: 0.00010 \t Train Loss: 0.01150 t Epoch Time: 75.5601 \t Total time: 6744.2863\n",
      "\n",
      "Epoch 93/200\n",
      "Epoch 93/200 \t Learning rate: 0.00010 \t Train Loss: 0.01501 t Epoch Time: 75.4507 \t Total time: 6819.7370\n",
      "\n",
      "Epoch 94/200\n",
      "Epoch 94/200 \t Learning rate: 0.00010 \t Train Loss: 0.00859 t Epoch Time: 72.4777 \t Total time: 6892.2147\n",
      "\n",
      "Epoch 95/200\n",
      "Epoch 95/200 \t Learning rate: 0.00010 \t Train Loss: 0.01439 t Epoch Time: 70.9424 \t Total time: 6963.1571\n",
      "\n",
      "Epoch 96/200\n",
      "Epoch 96/200 \t Learning rate: 0.00010 \t Train Loss: 0.00936 t Epoch Time: 70.8661 \t Total time: 7034.0232\n",
      "\n",
      "Epoch 97/200\n",
      "Epoch 97/200 \t Learning rate: 0.00010 \t Train Loss: 0.00795 t Epoch Time: 71.0243 \t Total time: 7105.0475\n",
      "\n",
      "Epoch 98/200\n",
      "Epoch 98/200 \t Learning rate: 0.00010 \t Train Loss: 0.01390 t Epoch Time: 71.0584 \t Total time: 7176.1060\n",
      "\n",
      "Epoch 99/200\n",
      "Epoch 99/200 \t Learning rate: 0.00010 \t Train Loss: 0.01315 t Epoch Time: 71.1593 \t Total time: 7247.2653\n",
      "\n",
      "Epoch 100/200\n",
      "Epoch 100/200 \t Learning rate: 0.00005 \t Train Loss: 0.00748 t Epoch Time: 71.1728 \t Total time: 7318.4381\n",
      "\n",
      "Epoch 101/200\n",
      "Epoch 101/200 \t Learning rate: 0.00005 \t Train Loss: 0.00869 t Epoch Time: 71.2614 \t Total time: 7389.6995\n",
      "\n",
      "Epoch 102/200\n",
      "Epoch 102/200 \t Learning rate: 0.00005 \t Train Loss: 0.00499 t Epoch Time: 71.2432 \t Total time: 7460.9426\n",
      "\n",
      "Epoch 103/200\n",
      "Epoch 103/200 \t Learning rate: 0.00005 \t Train Loss: 0.00815 t Epoch Time: 71.2605 \t Total time: 7532.2032\n",
      "\n",
      "Epoch 104/200\n",
      "Epoch 104/200 \t Learning rate: 0.00005 \t Train Loss: 0.00686 t Epoch Time: 71.1878 \t Total time: 7603.3909\n",
      "\n",
      "Epoch 105/200\n",
      "Epoch 105/200 \t Learning rate: 0.00005 \t Train Loss: 0.00545 t Epoch Time: 71.1431 \t Total time: 7674.5340\n",
      "\n",
      "Epoch 106/200\n",
      "Epoch 106/200 \t Learning rate: 0.00005 \t Train Loss: 0.00697 t Epoch Time: 71.1955 \t Total time: 7745.7295\n",
      "\n",
      "Epoch 107/200\n",
      "Epoch 107/200 \t Learning rate: 0.00005 \t Train Loss: 0.00906 t Epoch Time: 73.3095 \t Total time: 7819.0389\n",
      "\n",
      "Epoch 108/200\n",
      "Epoch 108/200 \t Learning rate: 0.00005 \t Train Loss: 0.00797 t Epoch Time: 74.5878 \t Total time: 7893.6268\n",
      "\n",
      "Epoch 109/200\n",
      "Epoch 109/200 \t Learning rate: 0.00005 \t Train Loss: 0.00648 t Epoch Time: 74.2517 \t Total time: 7967.8785\n",
      "\n",
      "Epoch 110/200\n",
      "Epoch 110/200 \t Learning rate: 0.00005 \t Train Loss: 0.00825 t Epoch Time: 74.1921 \t Total time: 8042.0706\n",
      "\n",
      "Epoch 111/200\n",
      "Epoch 111/200 \t Learning rate: 0.00005 \t Train Loss: 0.00809 t Epoch Time: 74.3674 \t Total time: 8116.4380\n",
      "\n",
      "Epoch 112/200\n",
      "Epoch 112/200 \t Learning rate: 0.00005 \t Train Loss: 0.00732 t Epoch Time: 74.1175 \t Total time: 8190.5554\n",
      "\n",
      "Epoch 113/200\n",
      "Epoch 113/200 \t Learning rate: 0.00005 \t Train Loss: 0.00798 t Epoch Time: 73.2682 \t Total time: 8263.8236\n",
      "\n",
      "Epoch 114/200\n",
      "Epoch 114/200 \t Learning rate: 0.00005 \t Train Loss: 0.00666 t Epoch Time: 71.3399 \t Total time: 8335.1635\n",
      "\n",
      "Epoch 115/200\n",
      "Epoch 115/200 \t Learning rate: 0.00005 \t Train Loss: 0.00624 t Epoch Time: 71.5065 \t Total time: 8406.6701\n",
      "\n",
      "Epoch 116/200\n",
      "Epoch 116/200 \t Learning rate: 0.00005 \t Train Loss: 0.00968 t Epoch Time: 71.8284 \t Total time: 8478.4985\n",
      "\n",
      "Epoch 117/200\n",
      "Epoch 117/200 \t Learning rate: 0.00005 \t Train Loss: 0.00817 t Epoch Time: 71.7886 \t Total time: 8550.2871\n",
      "\n",
      "Epoch 118/200\n",
      "Epoch 118/200 \t Learning rate: 0.00005 \t Train Loss: 0.00990 t Epoch Time: 74.0179 \t Total time: 8624.3050\n",
      "\n",
      "Epoch 119/200\n",
      "Epoch 119/200 \t Learning rate: 0.00005 \t Train Loss: 0.00588 t Epoch Time: 75.8470 \t Total time: 8700.1520\n",
      "\n",
      "Epoch 120/200\n",
      "Epoch 120/200 \t Learning rate: 0.00005 \t Train Loss: 0.00899 t Epoch Time: 75.0260 \t Total time: 8775.1780\n",
      "\n",
      "Epoch 121/200\n",
      "Epoch 121/200 \t Learning rate: 0.00005 \t Train Loss: 0.00998 t Epoch Time: 71.3732 \t Total time: 8846.5512\n",
      "\n",
      "Epoch 122/200\n",
      "Epoch 122/200 \t Learning rate: 0.00005 \t Train Loss: 0.00851 t Epoch Time: 72.6569 \t Total time: 8919.2081\n",
      "\n",
      "Epoch 123/200\n",
      "Epoch 123/200 \t Learning rate: 0.00005 \t Train Loss: 0.00947 t Epoch Time: 75.7832 \t Total time: 8994.9913\n",
      "\n",
      "Epoch 124/200\n",
      "Epoch 124/200 \t Learning rate: 0.00005 \t Train Loss: 0.00828 t Epoch Time: 72.7708 \t Total time: 9067.7621\n",
      "\n",
      "Epoch 125/200\n",
      "Epoch 125/200 \t Learning rate: 0.00005 \t Train Loss: 0.00682 t Epoch Time: 73.5951 \t Total time: 9141.3572\n",
      "\n",
      "Epoch 126/200\n",
      "Epoch 126/200 \t Learning rate: 0.00005 \t Train Loss: 0.00896 t Epoch Time: 73.9198 \t Total time: 9215.2770\n",
      "\n",
      "Epoch 127/200\n",
      "Epoch 127/200 \t Learning rate: 0.00005 \t Train Loss: 0.00763 t Epoch Time: 73.9905 \t Total time: 9289.2676\n",
      "\n",
      "Epoch 128/200\n",
      "Epoch 128/200 \t Learning rate: 0.00005 \t Train Loss: 0.00771 t Epoch Time: 74.0209 \t Total time: 9363.2885\n",
      "\n",
      "Epoch 129/200\n",
      "Epoch 129/200 \t Learning rate: 0.00005 \t Train Loss: 0.00437 t Epoch Time: 73.9781 \t Total time: 9437.2666\n",
      "\n",
      "Epoch 130/200\n",
      "Epoch 130/200 \t Learning rate: 0.00005 \t Train Loss: 0.00785 t Epoch Time: 74.0064 \t Total time: 9511.2730\n",
      "\n",
      "Epoch 131/200\n",
      "Epoch 131/200 \t Learning rate: 0.00005 \t Train Loss: 0.00865 t Epoch Time: 74.3424 \t Total time: 9585.6154\n",
      "\n",
      "Epoch 132/200\n",
      "Epoch 132/200 \t Learning rate: 0.00005 \t Train Loss: 0.00718 t Epoch Time: 73.4344 \t Total time: 9659.0499\n",
      "\n",
      "Epoch 133/200\n",
      "Epoch 133/200 \t Learning rate: 0.00005 \t Train Loss: 0.00763 t Epoch Time: 70.8462 \t Total time: 9729.8961\n",
      "\n",
      "Epoch 134/200\n",
      "Epoch 134/200 \t Learning rate: 0.00005 \t Train Loss: 0.00775 t Epoch Time: 74.1190 \t Total time: 9804.0150\n",
      "\n",
      "Epoch 135/200\n",
      "Epoch 135/200 \t Learning rate: 0.00005 \t Train Loss: 0.00823 t Epoch Time: 73.1965 \t Total time: 9877.2115\n",
      "\n",
      "Epoch 136/200\n",
      "Epoch 136/200 \t Learning rate: 0.00005 \t Train Loss: 0.00784 t Epoch Time: 72.8048 \t Total time: 9950.0163\n",
      "\n",
      "Epoch 137/200\n",
      "Epoch 137/200 \t Learning rate: 0.00005 \t Train Loss: 0.00877 t Epoch Time: 72.8302 \t Total time: 10022.8465\n",
      "\n",
      "Epoch 138/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/200 \t Learning rate: 0.00005 \t Train Loss: 0.00649 t Epoch Time: 72.8755 \t Total time: 10095.7220\n",
      "\n",
      "Epoch 139/200\n",
      "Epoch 139/200 \t Learning rate: 0.00005 \t Train Loss: 0.00806 t Epoch Time: 72.5203 \t Total time: 10168.2424\n",
      "\n",
      "Epoch 140/200\n",
      "Epoch 140/200 \t Learning rate: 0.00005 \t Train Loss: 0.00947 t Epoch Time: 71.1340 \t Total time: 10239.3763\n",
      "\n",
      "Epoch 141/200\n",
      "Epoch 141/200 \t Learning rate: 0.00005 \t Train Loss: 0.01058 t Epoch Time: 71.3833 \t Total time: 10310.7596\n",
      "\n",
      "Epoch 142/200\n",
      "Epoch 142/200 \t Learning rate: 0.00005 \t Train Loss: 0.00750 t Epoch Time: 71.3996 \t Total time: 10382.1592\n",
      "\n",
      "Epoch 143/200\n",
      "Epoch 143/200 \t Learning rate: 0.00005 \t Train Loss: 0.00658 t Epoch Time: 72.9460 \t Total time: 10455.1052\n",
      "\n",
      "Epoch 144/200\n",
      "Epoch 144/200 \t Learning rate: 0.00005 \t Train Loss: 0.00552 t Epoch Time: 73.0136 \t Total time: 10528.1188\n",
      "\n",
      "Epoch 145/200\n",
      "Epoch 145/200 \t Learning rate: 0.00005 \t Train Loss: 0.00721 t Epoch Time: 72.9555 \t Total time: 10601.0743\n",
      "\n",
      "Epoch 146/200\n",
      "Epoch 146/200 \t Learning rate: 0.00005 \t Train Loss: 0.00553 t Epoch Time: 72.9135 \t Total time: 10673.9878\n",
      "\n",
      "Epoch 147/200\n",
      "Epoch 147/200 \t Learning rate: 0.00005 \t Train Loss: 0.00671 t Epoch Time: 72.7556 \t Total time: 10746.7434\n",
      "\n",
      "Epoch 148/200\n",
      "Epoch 148/200 \t Learning rate: 0.00005 \t Train Loss: 0.00812 t Epoch Time: 72.7462 \t Total time: 10819.4896\n",
      "\n",
      "Epoch 149/200\n",
      "Epoch 149/200 \t Learning rate: 0.00005 \t Train Loss: 0.00543 t Epoch Time: 72.7828 \t Total time: 10892.2724\n",
      "\n",
      "Epoch 150/200\n",
      "Epoch 150/200 \t Learning rate: 0.00005 \t Train Loss: 0.00939 t Epoch Time: 73.2423 \t Total time: 10965.5148\n",
      "\n",
      "Epoch 151/200\n",
      "Epoch 151/200 \t Learning rate: 0.00005 \t Train Loss: 0.00571 t Epoch Time: 71.7175 \t Total time: 11037.2322\n",
      "\n",
      "Epoch 152/200\n",
      "Epoch 152/200 \t Learning rate: 0.00005 \t Train Loss: 0.00825 t Epoch Time: 71.1769 \t Total time: 11108.4091\n",
      "\n",
      "Epoch 153/200\n",
      "Epoch 153/200 \t Learning rate: 0.00005 \t Train Loss: 0.00581 t Epoch Time: 71.3753 \t Total time: 11179.7844\n",
      "\n",
      "Epoch 154/200\n",
      "Epoch 154/200 \t Learning rate: 0.00005 \t Train Loss: 0.00612 t Epoch Time: 71.4061 \t Total time: 11251.1905\n",
      "\n",
      "Epoch 155/200\n",
      "Epoch 155/200 \t Learning rate: 0.00005 \t Train Loss: 0.00640 t Epoch Time: 71.4318 \t Total time: 11322.6223\n",
      "\n",
      "Epoch 156/200\n",
      "Epoch 156/200 \t Learning rate: 0.00005 \t Train Loss: 0.00622 t Epoch Time: 73.2822 \t Total time: 11395.9045\n",
      "\n",
      "Epoch 157/200\n",
      "Epoch 157/200 \t Learning rate: 0.00005 \t Train Loss: 0.00772 t Epoch Time: 73.1474 \t Total time: 11469.0519\n",
      "\n",
      "Epoch 158/200\n",
      "Epoch 158/200 \t Learning rate: 0.00005 \t Train Loss: 0.00745 t Epoch Time: 73.1500 \t Total time: 11542.2019\n",
      "\n",
      "Epoch 159/200\n",
      "Epoch 159/200 \t Learning rate: 0.00005 \t Train Loss: 0.00848 t Epoch Time: 73.1834 \t Total time: 11615.3852\n",
      "\n",
      "Epoch 160/200\n",
      "Epoch 160/200 \t Learning rate: 0.00005 \t Train Loss: 0.00693 t Epoch Time: 73.0920 \t Total time: 11688.4772\n",
      "\n",
      "Epoch 161/200\n",
      "Epoch 161/200 \t Learning rate: 0.00005 \t Train Loss: 0.00951 t Epoch Time: 73.0712 \t Total time: 11761.5484\n",
      "\n",
      "Epoch 162/200\n",
      "Epoch 162/200 \t Learning rate: 0.00005 \t Train Loss: 0.00811 t Epoch Time: 73.2838 \t Total time: 11834.8322\n",
      "\n",
      "Epoch 163/200\n",
      "Epoch 163/200 \t Learning rate: 0.00005 \t Train Loss: 0.00888 t Epoch Time: 73.2854 \t Total time: 11908.1176\n",
      "\n",
      "Epoch 164/200\n",
      "Epoch 164/200 \t Learning rate: 0.00005 \t Train Loss: 0.00733 t Epoch Time: 73.1788 \t Total time: 11981.2964\n",
      "\n",
      "Epoch 165/200\n",
      "Epoch 165/200 \t Learning rate: 0.00005 \t Train Loss: 0.00704 t Epoch Time: 73.2375 \t Total time: 12054.5339\n",
      "\n",
      "Epoch 166/200\n",
      "Epoch 166/200 \t Learning rate: 0.00005 \t Train Loss: 0.00862 t Epoch Time: 74.0575 \t Total time: 12128.5914\n",
      "\n",
      "Epoch 167/200\n",
      "Epoch 167/200 \t Learning rate: 0.00005 \t Train Loss: 0.00718 t Epoch Time: 75.4412 \t Total time: 12204.0326\n",
      "\n",
      "Epoch 168/200\n",
      "Epoch 168/200 \t Learning rate: 0.00005 \t Train Loss: 0.00942 t Epoch Time: 75.3571 \t Total time: 12279.3898\n",
      "\n",
      "Epoch 169/200\n",
      "Epoch 169/200 \t Learning rate: 0.00005 \t Train Loss: 0.01126 t Epoch Time: 74.8101 \t Total time: 12354.1998\n",
      "\n",
      "Epoch 170/200\n",
      "Epoch 170/200 \t Learning rate: 0.00005 \t Train Loss: 0.00703 t Epoch Time: 72.4631 \t Total time: 12426.6630\n",
      "\n",
      "Epoch 171/200\n",
      "Epoch 171/200 \t Learning rate: 0.00005 \t Train Loss: 0.00806 t Epoch Time: 72.2525 \t Total time: 12498.9155\n",
      "\n",
      "Epoch 172/200\n",
      "Epoch 172/200 \t Learning rate: 0.00005 \t Train Loss: 0.00751 t Epoch Time: 72.5645 \t Total time: 12571.4800\n",
      "\n",
      "Epoch 173/200\n",
      "Epoch 173/200 \t Learning rate: 0.00005 \t Train Loss: 0.01190 t Epoch Time: 75.9121 \t Total time: 12647.3921\n",
      "\n",
      "Epoch 174/200\n",
      "Epoch 174/200 \t Learning rate: 0.00005 \t Train Loss: 0.00968 t Epoch Time: 74.8382 \t Total time: 12722.2303\n",
      "\n",
      "Epoch 175/200\n",
      "Epoch 175/200 \t Learning rate: 0.00005 \t Train Loss: 0.00645 t Epoch Time: 71.0962 \t Total time: 12793.3265\n",
      "\n",
      "Epoch 176/200\n",
      "Epoch 176/200 \t Learning rate: 0.00005 \t Train Loss: 0.00987 t Epoch Time: 74.1449 \t Total time: 12867.4714\n",
      "\n",
      "Epoch 177/200\n",
      "Epoch 177/200 \t Learning rate: 0.00005 \t Train Loss: 0.00597 t Epoch Time: 73.5210 \t Total time: 12940.9924\n",
      "\n",
      "Epoch 178/200\n",
      "Epoch 178/200 \t Learning rate: 0.00005 \t Train Loss: 0.00780 t Epoch Time: 72.9654 \t Total time: 13013.9577\n",
      "\n",
      "Epoch 179/200\n",
      "Epoch 179/200 \t Learning rate: 0.00005 \t Train Loss: 0.00770 t Epoch Time: 71.9163 \t Total time: 13085.8740\n",
      "\n",
      "Epoch 180/200\n",
      "Epoch 180/200 \t Learning rate: 0.00005 \t Train Loss: 0.00833 t Epoch Time: 74.3202 \t Total time: 13160.1942\n",
      "\n",
      "Epoch 181/200\n",
      "Epoch 181/200 \t Learning rate: 0.00005 \t Train Loss: 0.00727 t Epoch Time: 75.3615 \t Total time: 13235.5557\n",
      "\n",
      "Epoch 182/200\n",
      "Epoch 182/200 \t Learning rate: 0.00005 \t Train Loss: 0.01201 t Epoch Time: 74.0710 \t Total time: 13309.6268\n",
      "\n",
      "Epoch 183/200\n",
      "Epoch 183/200 \t Learning rate: 0.00005 \t Train Loss: 0.00692 t Epoch Time: 72.8907 \t Total time: 13382.5175\n",
      "\n",
      "Epoch 184/200\n",
      "Epoch 184/200 \t Learning rate: 0.00005 \t Train Loss: 0.00563 t Epoch Time: 73.3979 \t Total time: 13455.9154\n",
      "\n",
      "Epoch 185/200\n",
      "Epoch 185/200 \t Learning rate: 0.00005 \t Train Loss: 0.00541 t Epoch Time: 75.2075 \t Total time: 13531.1229\n",
      "\n",
      "Epoch 186/200\n",
      "Epoch 186/200 \t Learning rate: 0.00005 \t Train Loss: 0.00683 t Epoch Time: 75.6086 \t Total time: 13606.7315\n",
      "\n",
      "Epoch 187/200\n",
      "Epoch 187/200 \t Learning rate: 0.00005 \t Train Loss: 0.00737 t Epoch Time: 74.8343 \t Total time: 13681.5658\n",
      "\n",
      "Epoch 188/200\n",
      "Epoch 188/200 \t Learning rate: 0.00005 \t Train Loss: 0.00681 t Epoch Time: 71.9096 \t Total time: 13753.4755\n",
      "\n",
      "Epoch 189/200\n",
      "Epoch 189/200 \t Learning rate: 0.00005 \t Train Loss: 0.00645 t Epoch Time: 72.3841 \t Total time: 13825.8595\n",
      "\n",
      "Epoch 190/200\n",
      "Epoch 190/200 \t Learning rate: 0.00005 \t Train Loss: 0.00634 t Epoch Time: 72.4698 \t Total time: 13898.3294\n",
      "\n",
      "Epoch 191/200\n",
      "Epoch 191/200 \t Learning rate: 0.00005 \t Train Loss: 0.00791 t Epoch Time: 72.9851 \t Total time: 13971.3145\n",
      "\n",
      "Epoch 192/200\n",
      "Epoch 192/200 \t Learning rate: 0.00005 \t Train Loss: 0.00735 t Epoch Time: 72.9300 \t Total time: 14044.2444\n",
      "\n",
      "Epoch 193/200\n",
      "Epoch 193/200 \t Learning rate: 0.00005 \t Train Loss: 0.01027 t Epoch Time: 72.9115 \t Total time: 14117.1559\n",
      "\n",
      "Epoch 194/200\n",
      "Epoch 194/200 \t Learning rate: 0.00005 \t Train Loss: 0.00666 t Epoch Time: 72.8668 \t Total time: 14190.0227\n",
      "\n",
      "Epoch 195/200\n",
      "Epoch 195/200 \t Learning rate: 0.00005 \t Train Loss: 0.00786 t Epoch Time: 72.8358 \t Total time: 14262.8585\n",
      "\n",
      "Epoch 196/200\n",
      "Epoch 196/200 \t Learning rate: 0.00005 \t Train Loss: 0.00892 t Epoch Time: 72.4409 \t Total time: 14335.2994\n",
      "\n",
      "Epoch 197/200\n",
      "Epoch 197/200 \t Learning rate: 0.00005 \t Train Loss: 0.00614 t Epoch Time: 71.3764 \t Total time: 14406.6758\n",
      "\n",
      "Epoch 198/200\n",
      "Epoch 198/200 \t Learning rate: 0.00005 \t Train Loss: 0.00748 t Epoch Time: 71.0816 \t Total time: 14477.7573\n",
      "\n",
      "Epoch 199/200\n",
      "Epoch 199/200 \t Learning rate: 0.00005 \t Train Loss: 0.00650 t Epoch Time: 73.1059 \t Total time: 14550.8632\n",
      "\n",
      "Epoch 200/200\n",
      "Epoch 200/200 \t Learning rate: 0.00003 \t Train Loss: 0.00803 t Epoch Time: 73.0507 \t Total time: 14623.9139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Test DB...\n",
      "Loading the Model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading Test DB...\")\n",
    "test_dataset = triplet_test(args.test_dir, resize=None)\n",
    "\n",
    "print(\"Loading the Model...\")\n",
    "ckptDir = args.save_dir + '/chekpoint'\n",
    "checkpoint = torch.load(ckptDir + '/model_epoch010.pth', map_location=torch.device('cpu'))\n",
    "model = ConvNetSep(kernel_size=args.kernel_size)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_resultDir_final = args.save_dir + '/test_result_final'\n",
    "if not os.path.exists(test_resultDir_final):\n",
    "        os.makedirs(test_resultDir_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Start...\n",
      "PSNR on test data: 33.230865649459886\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Start...\")\n",
    "test_psnr = test(model, test_dataset, test_resultDir_final, output_name='output.png')\n",
    "test_log = \"PSNR on test data: {}\".format(test_psnr)\n",
    "print(test_log)\n",
    "\n",
    "log_dir = args.save_dir + '/logging'\n",
    "\n",
    "if os.path.exists(log_dir + '/log_test.txt'):\n",
    "    logfile = open(log_dir + '/log_test.txt', 'a')\n",
    "else:\n",
    "    logfile = open(log_dir + '/log_test.txt', 'w')\n",
    "        \n",
    "logfile.write(test_log)\n",
    "logfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
