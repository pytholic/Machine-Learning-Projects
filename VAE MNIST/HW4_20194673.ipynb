{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, device, weight):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var, weight)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, weight):\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var, weight).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Design the autoencoder structured network for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE!!\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        \n",
    "        #############################################################\n",
    "    \n",
    "        # YOUR CODE!!\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc3_1 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc3_2 = nn.Linear(h_dim2, z_dim)\n",
    "\n",
    "        #############################################################\n",
    "        \n",
    "        \n",
    "        # decoder part\n",
    "        \n",
    "        #############################################################\n",
    "    \n",
    "        # YOUR CODE!!\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        #############################################################\n",
    "        \n",
    "        \n",
    "    def encoder(self, x):\n",
    "        # return mu, log_var\n",
    "        \n",
    "        #############################################################\n",
    "    \n",
    "        # YOUR CODE!!\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc3_1(h), self.fc3_2(h) # mu, log_var\n",
    "        #############################################################\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        # return z sample\n",
    "        \n",
    "        #############################################################\n",
    "    \n",
    "        # YOUR CODE!!\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        #############################################################\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        # return generated img\n",
    "        \n",
    "        #############################################################\n",
    "    \n",
    "        # YOUR CODE!!\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h))\n",
    "        #############################################################\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=50)\n",
    "if torch.cuda.is_available():\n",
    "    vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3_1): Linear(in_features=256, out_features=50, bias=True)\n",
       "  (fc3_2): Linear(in_features=256, out_features=50, bias=True)\n",
       "  (fc4): Linear(in_features=50, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Design the loss function for autoencoder with weight of KLD term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Without weight term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(recon_x, x, mu, log_var, weight):\n",
    "    # return reconstruction error + KL divergence losses\n",
    "    \n",
    "    #############################################################\n",
    "    \n",
    "    # YOUR CODE!!\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "    #############################################################\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4333dcd5ed3e4bff8f97b321f5e5576b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\trojan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 545.821350\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 192.805695\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 192.197800\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 164.205566\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 157.557327\n",
      "====> Epoch: 1 Average loss: 182.2670\n",
      "====> Test set loss: 145.4146\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 142.497513\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 143.637848\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 137.245468\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 125.718460\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 123.662743\n",
      "====> Epoch: 2 Average loss: 134.3210\n",
      "====> Test set loss: 125.1247\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 126.740646\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 120.118423\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 114.109962\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 121.798378\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 114.059082\n",
      "====> Epoch: 3 Average loss: 120.3644\n",
      "====> Test set loss: 116.5673\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 117.849380\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 119.976997\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 114.036224\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 104.910431\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 112.878891\n",
      "====> Epoch: 4 Average loss: 114.8764\n",
      "====> Test set loss: 112.1863\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 109.692406\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 111.043663\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 112.396088\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 107.799530\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 108.885620\n",
      "====> Epoch: 5 Average loss: 111.6404\n",
      "====> Test set loss: 109.5952\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 107.931358\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 105.476700\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 108.110970\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 108.350632\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 110.298660\n",
      "====> Epoch: 6 Average loss: 109.4455\n",
      "====> Test set loss: 107.8552\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 106.651657\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 109.745155\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 114.520439\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 107.010010\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 112.602486\n",
      "====> Epoch: 7 Average loss: 107.9190\n",
      "====> Test set loss: 107.1804\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 110.075706\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 109.338486\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 107.838516\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 110.704201\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 109.862740\n",
      "====> Epoch: 8 Average loss: 106.6841\n",
      "====> Test set loss: 105.9575\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 107.557678\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 110.006165\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 105.969452\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 103.445374\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 109.997543\n",
      "====> Epoch: 9 Average loss: 105.7612\n",
      "====> Test set loss: 105.3365\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 104.296417\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 101.210999\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 102.418129\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 103.658264\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 103.507637\n",
      "====> Epoch: 10 Average loss: 104.9911\n",
      "====> Test set loss: 104.6850\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 106.887497\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 104.295082\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 105.056503\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 104.941284\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 100.797432\n",
      "====> Epoch: 11 Average loss: 104.2200\n",
      "====> Test set loss: 103.9184\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 104.899902\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 105.583740\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 103.117264\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 101.565308\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 102.789261\n",
      "====> Epoch: 12 Average loss: 103.4692\n",
      "====> Test set loss: 103.2628\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 107.293243\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 103.725250\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 102.705231\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 101.033180\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 105.694519\n",
      "====> Epoch: 13 Average loss: 102.8044\n",
      "====> Test set loss: 102.5894\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 105.667114\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 104.953796\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 100.214447\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 100.844933\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 100.279190\n",
      "====> Epoch: 14 Average loss: 102.3491\n",
      "====> Test set loss: 102.3423\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 99.519638\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 104.248337\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 101.941650\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 101.932739\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 101.114365\n",
      "====> Epoch: 15 Average loss: 101.9024\n",
      "====> Test set loss: 102.3943\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 102.709625\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 102.216957\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 99.565193\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 106.494217\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 100.697769\n",
      "====> Epoch: 16 Average loss: 101.5396\n",
      "====> Test set loss: 101.9630\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 100.292534\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 99.395752\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 104.858505\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 102.515778\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 101.610718\n",
      "====> Epoch: 17 Average loss: 101.1499\n",
      "====> Test set loss: 101.7252\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 99.483002\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 100.817528\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 96.965828\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 104.276001\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 102.309570\n",
      "====> Epoch: 18 Average loss: 100.9127\n",
      "====> Test set loss: 102.2549\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 101.092117\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 104.560944\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 99.995087\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 99.641502\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 103.466629\n",
      "====> Epoch: 19 Average loss: 100.6478\n",
      "====> Test set loss: 101.1971\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 103.711288\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 98.051323\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 104.176613\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 101.712845\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 100.313202\n",
      "====> Epoch: 20 Average loss: 100.3822\n",
      "====> Test set loss: 100.9989\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 97.582443\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 99.608643\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 99.048958\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 101.143257\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 95.029160\n",
      "====> Epoch: 21 Average loss: 100.1169\n",
      "====> Test set loss: 100.9685\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 97.481537\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 100.227036\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 103.215240\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 100.966255\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 101.753326\n",
      "====> Epoch: 22 Average loss: 99.7897\n",
      "====> Test set loss: 100.4355\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 97.518250\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 97.263725\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 98.569633\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 101.312035\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 100.826859\n",
      "====> Epoch: 23 Average loss: 99.6014\n",
      "====> Test set loss: 100.6073\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 98.579300\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 95.499428\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 100.835533\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 102.162048\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 104.076996\n",
      "====> Epoch: 24 Average loss: 99.3635\n",
      "====> Test set loss: 100.2517\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 98.951141\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 98.449539\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 100.184906\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 96.927498\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 98.376755\n",
      "====> Epoch: 25 Average loss: 99.1891\n",
      "====> Test set loss: 100.0347\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 98.616455\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 98.508362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 98.971985\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 98.550209\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 100.730392\n",
      "====> Epoch: 26 Average loss: 99.0468\n",
      "====> Test set loss: 100.1467\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 100.841766\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 100.818016\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 101.850670\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 99.892288\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 100.751129\n",
      "====> Epoch: 27 Average loss: 98.8758\n",
      "====> Test set loss: 100.1053\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 93.275352\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 97.266846\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 97.685120\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 99.878029\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 98.906952\n",
      "====> Epoch: 28 Average loss: 98.7179\n",
      "====> Test set loss: 99.9514\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 97.477043\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 98.623230\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 99.972351\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 94.248978\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 101.041794\n",
      "====> Epoch: 29 Average loss: 98.5609\n",
      "====> Test set loss: 99.9086\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 94.857040\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 97.399872\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 102.476433\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 99.212692\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 95.173965\n",
      "====> Epoch: 30 Average loss: 98.4960\n",
      "====> Test set loss: 99.8539\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 96.592979\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 102.673904\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 96.575546\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 96.264450\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 98.698547\n",
      "====> Epoch: 31 Average loss: 98.3548\n",
      "====> Test set loss: 99.9583\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 98.662796\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 96.541489\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 97.404030\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 99.293755\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 99.515427\n",
      "====> Epoch: 32 Average loss: 98.2426\n",
      "====> Test set loss: 99.8958\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 98.110298\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 102.834625\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 99.916229\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 94.280151\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 95.890945\n",
      "====> Epoch: 33 Average loss: 98.1405\n",
      "====> Test set loss: 99.5314\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 99.619553\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 97.344406\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 95.624641\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 95.945801\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 99.861855\n",
      "====> Epoch: 34 Average loss: 97.9777\n",
      "====> Test set loss: 99.6461\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 97.737427\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 99.756088\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 98.036392\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 97.224289\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 94.617386\n",
      "====> Epoch: 35 Average loss: 97.9503\n",
      "====> Test set loss: 99.4080\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 96.697617\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 97.080376\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 99.608521\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 99.125008\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 98.837700\n",
      "====> Epoch: 36 Average loss: 97.8976\n",
      "====> Test set loss: 99.5897\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 97.213676\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 97.232544\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 97.352600\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 97.007347\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 97.725800\n",
      "====> Epoch: 37 Average loss: 97.8074\n",
      "====> Test set loss: 99.4181\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 97.221321\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 91.917664\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 97.190086\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 95.965546\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 96.079460\n",
      "====> Epoch: 38 Average loss: 97.6307\n",
      "====> Test set loss: 99.1856\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 98.650978\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 98.820206\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 97.970108\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 97.514938\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 97.020393\n",
      "====> Epoch: 39 Average loss: 97.6076\n",
      "====> Test set loss: 99.1253\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 96.837860\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 97.899124\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 95.944656\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 97.859497\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 94.681519\n",
      "====> Epoch: 40 Average loss: 97.5239\n",
      "====> Test set loss: 99.5762\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 100.970688\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 98.549706\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 100.156853\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 94.978516\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 91.858353\n",
      "====> Epoch: 41 Average loss: 97.4250\n",
      "====> Test set loss: 99.1047\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 97.800827\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 95.696716\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 92.878784\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 94.132889\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 96.767868\n",
      "====> Epoch: 42 Average loss: 97.3828\n",
      "====> Test set loss: 99.2018\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 97.998055\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 99.477501\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 95.493874\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 98.062050\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 98.523300\n",
      "====> Epoch: 43 Average loss: 97.3253\n",
      "====> Test set loss: 99.2774\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 97.378250\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 98.134232\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 96.381271\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 96.146439\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 99.531311\n",
      "====> Epoch: 44 Average loss: 97.2669\n",
      "====> Test set loss: 99.1357\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 100.050812\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 93.082970\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 97.883453\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 95.845909\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 94.582146\n",
      "====> Epoch: 45 Average loss: 97.2422\n",
      "====> Test set loss: 99.3282\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 96.794815\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 93.008415\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 97.448441\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 97.683105\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 94.789986\n",
      "====> Epoch: 46 Average loss: 97.1129\n",
      "====> Test set loss: 99.1308\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 93.053940\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 94.005768\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 95.936630\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 99.364136\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 97.216011\n",
      "====> Epoch: 47 Average loss: 97.0755\n",
      "====> Test set loss: 99.2310\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 96.836685\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 94.283524\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 95.472969\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 100.629326\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 94.658501\n",
      "====> Epoch: 48 Average loss: 97.0144\n",
      "====> Test set loss: 99.4318\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 96.115692\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 97.468094\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 95.601128\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 98.375031\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 99.016388\n",
      "====> Epoch: 49 Average loss: 96.9393\n",
      "====> Test set loss: 99.0349\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 97.737625\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 96.710495\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 96.902992\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 96.348358\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 92.944145\n",
      "====> Epoch: 50 Average loss: 96.9029\n",
      "====> Test set loss: 98.9182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "for epoch in tqdm(range(1, 51)):\n",
    "    train(epoch, device, weight)\n",
    "    test(device, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #With weight term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, log_var, weight):\n",
    "    # return reconstruction error + weight * KL divergence losses\n",
    "    \n",
    "    #############################################################\n",
    "    \n",
    "    # YOUR CODE!!\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + weight*KLD\n",
    "    #############################################################\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d412d1463543fb87e01a48c010b05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 544.758545\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 193.983505\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 179.778244\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 167.685806\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 147.305252\n",
      "====> Epoch: 1 Average loss: 177.5859\n",
      "====> Test set loss: 137.9021\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 134.590347\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 129.942276\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 132.193085\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 123.798737\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 120.639763\n",
      "====> Epoch: 2 Average loss: 124.8393\n",
      "====> Test set loss: 114.1375\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 111.959023\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 106.714256\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 113.893631\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 112.939980\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 107.462067\n",
      "====> Epoch: 3 Average loss: 111.1574\n",
      "====> Test set loss: 106.9192\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 108.983452\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 110.404526\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 110.073578\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 104.027130\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 110.267502\n",
      "====> Epoch: 4 Average loss: 106.1756\n",
      "====> Test set loss: 103.3382\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 107.766571\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 100.056976\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 109.906204\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 103.366142\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 105.672951\n",
      "====> Epoch: 5 Average loss: 102.6439\n",
      "====> Test set loss: 100.8953\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 103.154152\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 109.513382\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 100.430359\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 103.151337\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 105.380341\n",
      "====> Epoch: 6 Average loss: 100.5310\n",
      "====> Test set loss: 99.4027\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 98.928360\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 96.301025\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 99.748688\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 99.683495\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 97.078857\n",
      "====> Epoch: 7 Average loss: 99.0497\n",
      "====> Test set loss: 97.9786\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 101.704727\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 95.664925\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 94.646591\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 97.813972\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 100.825363\n",
      "====> Epoch: 8 Average loss: 97.9548\n",
      "====> Test set loss: 97.3842\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 96.786163\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 94.317657\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 98.494621\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 99.137962\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 91.288391\n",
      "====> Epoch: 9 Average loss: 97.0493\n",
      "====> Test set loss: 96.3237\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 95.928772\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 94.459602\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 98.260719\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 96.104881\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 95.321548\n",
      "====> Epoch: 10 Average loss: 96.3197\n",
      "====> Test set loss: 96.1948\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 91.432808\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 97.572563\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 97.443039\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 98.410896\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 97.563072\n",
      "====> Epoch: 11 Average loss: 95.6684\n",
      "====> Test set loss: 95.4801\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 94.602669\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 95.169228\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 91.723740\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 99.309036\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 94.395645\n",
      "====> Epoch: 12 Average loss: 95.0806\n",
      "====> Test set loss: 95.0970\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 95.201202\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 96.410736\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 94.253166\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 98.388351\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 94.962906\n",
      "====> Epoch: 13 Average loss: 94.5158\n",
      "====> Test set loss: 94.7249\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 91.705971\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 91.573242\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 90.693993\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 90.190979\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 92.426605\n",
      "====> Epoch: 14 Average loss: 94.0908\n",
      "====> Test set loss: 94.5079\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 95.678139\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 92.232613\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 91.438477\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 98.017021\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 93.899712\n",
      "====> Epoch: 15 Average loss: 93.7625\n",
      "====> Test set loss: 94.2670\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 93.429222\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 93.906189\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 89.591423\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 94.318237\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 93.197418\n",
      "====> Epoch: 16 Average loss: 93.3803\n",
      "====> Test set loss: 93.9413\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 97.184616\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 94.466507\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 94.053055\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 92.630447\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 90.942345\n",
      "====> Epoch: 17 Average loss: 93.1450\n",
      "====> Test set loss: 93.9891\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 91.132462\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 94.029175\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 90.153450\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 91.935196\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 93.301796\n",
      "====> Epoch: 18 Average loss: 92.8964\n",
      "====> Test set loss: 93.4240\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 96.670624\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 92.016365\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 95.689117\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 92.276245\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 90.401382\n",
      "====> Epoch: 19 Average loss: 92.6569\n",
      "====> Test set loss: 93.4138\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 92.083069\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 95.127953\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 92.948845\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 94.252731\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 90.134308\n",
      "====> Epoch: 20 Average loss: 92.4273\n",
      "====> Test set loss: 93.3404\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 90.757294\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 91.420181\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 91.532578\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 93.867859\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 96.269562\n",
      "====> Epoch: 21 Average loss: 92.1826\n",
      "====> Test set loss: 93.0390\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 89.465820\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 92.398567\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 90.706635\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 91.651794\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 87.361015\n",
      "====> Epoch: 22 Average loss: 92.0494\n",
      "====> Test set loss: 93.2417\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 91.162727\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 92.157501\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 86.983688\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 92.197647\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 92.106964\n",
      "====> Epoch: 23 Average loss: 91.8427\n",
      "====> Test set loss: 92.8260\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 94.903816\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 91.145821\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 93.373337\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 90.267250\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 92.579025\n",
      "====> Epoch: 24 Average loss: 91.6890\n",
      "====> Test set loss: 92.8061\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 91.038620\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 91.804527\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 88.706985\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 90.756561\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 93.575462\n",
      "====> Epoch: 25 Average loss: 91.5470\n",
      "====> Test set loss: 92.5751\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 89.746742\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 88.974838\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 92.301483\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 90.343674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 94.353508\n",
      "====> Epoch: 26 Average loss: 91.4013\n",
      "====> Test set loss: 92.7872\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 94.463806\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 92.187889\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 93.612259\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 91.419830\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 91.161423\n",
      "====> Epoch: 27 Average loss: 91.3030\n",
      "====> Test set loss: 92.4349\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 86.154785\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 92.118546\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 89.845802\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 93.167534\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 88.984200\n",
      "====> Epoch: 28 Average loss: 91.1649\n",
      "====> Test set loss: 92.6114\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 90.739441\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 87.735016\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 91.387077\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 88.754829\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 92.391418\n",
      "====> Epoch: 29 Average loss: 91.0124\n",
      "====> Test set loss: 92.5491\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 92.578224\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 95.247292\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 92.970459\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 92.480354\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 89.471878\n",
      "====> Epoch: 30 Average loss: 90.9459\n",
      "====> Test set loss: 92.3290\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 88.854507\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 90.314987\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 88.526413\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 91.151482\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 91.437393\n",
      "====> Epoch: 31 Average loss: 90.8264\n",
      "====> Test set loss: 92.2155\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 92.079086\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 89.396881\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 93.355141\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 91.972107\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 92.512062\n",
      "====> Epoch: 32 Average loss: 90.7647\n",
      "====> Test set loss: 92.3680\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 88.968292\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 92.285400\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 87.810287\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 91.393509\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 91.341858\n",
      "====> Epoch: 33 Average loss: 90.6174\n",
      "====> Test set loss: 92.1141\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 85.684692\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 90.156647\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 94.597160\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 91.839859\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 91.068253\n",
      "====> Epoch: 34 Average loss: 90.5020\n",
      "====> Test set loss: 92.3468\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 90.140717\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 87.105057\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 92.787201\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 90.500893\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 91.044205\n",
      "====> Epoch: 35 Average loss: 90.4450\n",
      "====> Test set loss: 92.2921\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 88.790741\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 89.198349\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 89.027290\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 92.849434\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 95.493164\n",
      "====> Epoch: 36 Average loss: 90.3640\n",
      "====> Test set loss: 92.0730\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 89.593094\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 87.512962\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 90.630119\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 92.994812\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 94.533447\n",
      "====> Epoch: 37 Average loss: 90.3156\n",
      "====> Test set loss: 92.0582\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 92.587936\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 85.846008\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 89.019569\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 91.079628\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 89.861725\n",
      "====> Epoch: 38 Average loss: 90.2183\n",
      "====> Test set loss: 91.9141\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 90.189476\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 90.456818\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 92.727081\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 92.590813\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 89.430687\n",
      "====> Epoch: 39 Average loss: 90.1002\n",
      "====> Test set loss: 91.9501\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 90.019356\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 92.181580\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 85.857246\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 89.374474\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 87.373627\n",
      "====> Epoch: 40 Average loss: 90.0487\n",
      "====> Test set loss: 91.8561\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 86.384293\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 92.572281\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 95.632881\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 91.182251\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 90.660660\n",
      "====> Epoch: 41 Average loss: 89.9809\n",
      "====> Test set loss: 91.9744\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 86.334030\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 87.582458\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 88.084305\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 87.708206\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 90.877228\n",
      "====> Epoch: 42 Average loss: 89.8780\n",
      "====> Test set loss: 91.8642\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 87.998802\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 86.839767\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 91.020332\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 91.503326\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 94.233406\n",
      "====> Epoch: 43 Average loss: 89.8851\n",
      "====> Test set loss: 92.1012\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 88.033630\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 90.986298\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 91.539452\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 88.174988\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 89.538948\n",
      "====> Epoch: 44 Average loss: 89.8398\n",
      "====> Test set loss: 91.7107\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 93.664207\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 89.294861\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 91.016472\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 89.780327\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 91.999031\n",
      "====> Epoch: 45 Average loss: 89.7660\n",
      "====> Test set loss: 92.0041\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 88.418663\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 89.164368\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 92.089676\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 89.346802\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 89.571899\n",
      "====> Epoch: 46 Average loss: 89.6473\n",
      "====> Test set loss: 91.6187\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 87.215546\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 88.356216\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 89.443619\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 88.215637\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 89.645096\n",
      "====> Epoch: 47 Average loss: 89.6268\n",
      "====> Test set loss: 91.7322\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 87.435493\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 88.756165\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 89.496750\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 89.972473\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 89.721710\n",
      "====> Epoch: 48 Average loss: 89.5959\n",
      "====> Test set loss: 91.9225\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 89.560875\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 88.428886\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 88.890121\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 91.718224\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 85.125252\n",
      "====> Epoch: 49 Average loss: 89.5422\n",
      "====> Test set loss: 91.8247\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 91.780647\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 91.961533\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 85.865067\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 88.928345\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 89.843025\n",
      "====> Epoch: 50 Average loss: 89.4592\n",
      "====> Test set loss: 91.5742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# YOU HAVE TO CONTROL THE \"weight\" Variable!\n",
    "\n",
    "###############################################\n",
    "\n",
    "weight = 0.7 # change the value,\n",
    "\n",
    "###############################################\n",
    "\n",
    "for epoch in tqdm(range(1, 51)):\n",
    "    train(epoch, device, weight)\n",
    "    test(device, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence with weight = 0.7 is faster than no weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-d4dcb112dea7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "    # YOUR CODE!!\n",
    "    \n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "            \n",
    "    z = torch.tensor(z).to(device)\n",
    "    sample = vae.decoder(z.float())\n",
    "    \n",
    "    if not os.exists('./samples'):\n",
    "        os.makedirs('./samples')\n",
    "    \n",
    "    save_image(sample.view(64, 1, 28, 28), './samples/sample' + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "    # YOUR CODE!!\n",
    "    z = torch.randn(64, 50).to(device)\n",
    "    sample = vae.decoder(z).to(device)\n",
    "    \n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "            \n",
    "    #z = torch.tensor(z).to(device)\n",
    "    #sample = vae.decoder(z.float())\n",
    "    \n",
    "    if not os.path.exists('./samples'):\n",
    "        os.makedirs('./samples')\n",
    "    \n",
    "    save_image(sample.view(64, 1, 28, 28), './samples/sample_50D' + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
