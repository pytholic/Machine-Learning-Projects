{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, device, weight):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var, weight)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, weight):\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var, weight).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Design the autoencoder structured network for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE!!\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        \n",
    "        #############################################################\n",
    "    \n",
    "        # YOUR CODE!!\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc3_1 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc3_2 = nn.Linear(h_dim2, z_dim)\n",
    "\n",
    "        #############################################################\n",
    "        \n",
    "        \n",
    "        # decoder part\n",
    "        \n",
    "        #############################################################\n",
    "    \n",
    "        # YOUR CODE!!\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        #############################################################\n",
    "        \n",
    "        \n",
    "    def encoder(self, x):\n",
    "        # return mu, log_var\n",
    "        \n",
    "        #############################################################\n",
    "    \n",
    "        # YOUR CODE!!\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc3_1(h), self.fc3_2(h) # mu, log_var\n",
    "        #############################################################\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        # return z sample\n",
    "        \n",
    "        #############################################################\n",
    "    \n",
    "        # YOUR CODE!!\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        #############################################################\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        # return generated img\n",
    "        \n",
    "        #############################################################\n",
    "    \n",
    "        # YOUR CODE!!\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h))\n",
    "        #############################################################\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=50)\n",
    "if torch.cuda.is_available():\n",
    "    vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3_1): Linear(in_features=256, out_features=50, bias=True)\n",
       "  (fc3_2): Linear(in_features=256, out_features=50, bias=True)\n",
       "  (fc4): Linear(in_features=50, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Design the loss function for autoencoder with weight of KLD term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(recon_x, x, mu, log_var, weight):\n",
    "    # return reconstruction error + KL divergence losses\n",
    "    \n",
    "    #############################################################\n",
    "    \n",
    "    # YOUR CODE!!\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + weight*KLD\n",
    "    #############################################################\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288fd4e0767a4dce8b479c6978acacb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 543.653625\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 199.833420\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 175.766281\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 160.482452\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 149.153885\n",
      "====> Epoch: 1 Average loss: 178.2026\n",
      "====> Test set loss: 142.8234\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 143.130768\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 134.891724\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 139.952286\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 129.791550\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 123.460632\n",
      "====> Epoch: 2 Average loss: 132.3210\n",
      "====> Test set loss: 123.7309\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 126.979462\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 122.342552\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 122.273315\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 115.168045\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 113.036797\n",
      "====> Epoch: 3 Average loss: 119.9631\n",
      "====> Test set loss: 115.4444\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 117.260544\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 119.630295\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 114.232857\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 116.432411\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 116.751930\n",
      "====> Epoch: 4 Average loss: 114.2481\n",
      "====> Test set loss: 111.7911\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 109.248856\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 111.436981\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 109.470177\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 107.432945\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 115.874252\n",
      "====> Epoch: 5 Average loss: 111.2220\n",
      "====> Test set loss: 109.1984\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 113.110168\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 112.819702\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 110.096054\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 110.574463\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 111.219414\n",
      "====> Epoch: 6 Average loss: 108.8526\n",
      "====> Test set loss: 107.4432\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 106.880493\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 105.218605\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 104.658615\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 107.001289\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 109.281929\n",
      "====> Epoch: 7 Average loss: 107.1075\n",
      "====> Test set loss: 105.9531\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 104.548393\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 104.821701\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 103.981499\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 101.139740\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 105.814568\n",
      "====> Epoch: 8 Average loss: 105.8969\n",
      "====> Test set loss: 105.1582\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 105.754562\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 105.360245\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 107.873611\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 107.647209\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 105.283478\n",
      "====> Epoch: 9 Average loss: 104.9341\n",
      "====> Test set loss: 104.4805\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 109.924637\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 105.844879\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 106.151398\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 108.193398\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 98.389107\n",
      "====> Epoch: 10 Average loss: 104.1572\n",
      "====> Test set loss: 103.7627\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 103.157532\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 104.620224\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 103.304916\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 105.797363\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 100.797615\n",
      "====> Epoch: 11 Average loss: 103.5508\n",
      "====> Test set loss: 103.5600\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 104.018402\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 105.816299\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 103.427002\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 101.113770\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 98.694588\n",
      "====> Epoch: 12 Average loss: 103.0151\n",
      "====> Test set loss: 103.1518\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 102.924072\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 102.213364\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 103.440277\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 99.114555\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 97.874619\n",
      "====> Epoch: 13 Average loss: 102.5538\n",
      "====> Test set loss: 102.8196\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 100.922935\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 103.686874\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 99.357155\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 103.501694\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 100.008774\n",
      "====> Epoch: 14 Average loss: 102.0851\n",
      "====> Test set loss: 102.4738\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 101.418213\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 106.553665\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 98.430603\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 102.181587\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 98.270638\n",
      "====> Epoch: 15 Average loss: 101.7770\n",
      "====> Test set loss: 102.0854\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 99.481102\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 98.455627\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 106.729591\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 103.595474\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 99.381699\n",
      "====> Epoch: 16 Average loss: 101.4158\n",
      "====> Test set loss: 102.0994\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 104.798584\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 101.752396\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 98.881485\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 104.246033\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 102.228188\n",
      "====> Epoch: 17 Average loss: 101.1355\n",
      "====> Test set loss: 101.7606\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 101.446564\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 101.871002\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 98.375427\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 98.219589\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 102.473076\n",
      "====> Epoch: 18 Average loss: 100.8248\n",
      "====> Test set loss: 101.8125\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 98.691505\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 101.644547\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 107.133156\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 99.140900\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 99.944946\n",
      "====> Epoch: 19 Average loss: 100.5918\n",
      "====> Test set loss: 101.1382\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 97.400345\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 100.005386\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 100.938156\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 101.475990\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 99.915375\n",
      "====> Epoch: 20 Average loss: 100.3765\n",
      "====> Test set loss: 100.8918\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 103.694366\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 102.339920\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 100.907318\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 101.479050\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 101.442734\n",
      "====> Epoch: 21 Average loss: 100.1608\n",
      "====> Test set loss: 100.8095\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 103.442184\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 103.415817\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 100.384071\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 97.577057\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 95.504547\n",
      "====> Epoch: 22 Average loss: 99.9329\n",
      "====> Test set loss: 100.7463\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 102.147308\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 97.344048\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 98.514389\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 100.371666\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 103.113678\n",
      "====> Epoch: 23 Average loss: 99.6691\n",
      "====> Test set loss: 100.7184\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 103.731354\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 102.398407\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 99.819656\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 102.687500\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 96.114700\n",
      "====> Epoch: 24 Average loss: 99.5244\n",
      "====> Test set loss: 100.5434\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 101.657433\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 97.743294\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 105.088638\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 100.271072\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 95.205048\n",
      "====> Epoch: 25 Average loss: 99.3148\n",
      "====> Test set loss: 100.6303\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 96.505127\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 95.683311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 97.691620\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 96.240356\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 99.888481\n",
      "====> Epoch: 26 Average loss: 99.1719\n",
      "====> Test set loss: 100.3470\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 98.523796\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 93.197357\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 98.858337\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 98.419098\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 97.275337\n",
      "====> Epoch: 27 Average loss: 99.0315\n",
      "====> Test set loss: 100.2639\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 96.671700\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 95.544449\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 97.994720\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 96.478973\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 95.185966\n",
      "====> Epoch: 28 Average loss: 98.8812\n",
      "====> Test set loss: 100.3864\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 102.414253\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 94.755432\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 96.566345\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 98.803185\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 100.829010\n",
      "====> Epoch: 29 Average loss: 98.7591\n",
      "====> Test set loss: 100.2273\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 101.138824\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 98.541153\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 95.029037\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 99.142303\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 101.669075\n",
      "====> Epoch: 30 Average loss: 98.5875\n",
      "====> Test set loss: 99.9982\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 98.864647\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 93.662827\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 98.594208\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 100.425476\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 101.537140\n",
      "====> Epoch: 31 Average loss: 98.4487\n",
      "====> Test set loss: 99.9561\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 100.080917\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 99.327866\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 98.510040\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 99.791420\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 100.439095\n",
      "====> Epoch: 32 Average loss: 98.4141\n",
      "====> Test set loss: 99.4911\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 93.966225\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 97.956451\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 98.312927\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 96.459839\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 100.444458\n",
      "====> Epoch: 33 Average loss: 98.2799\n",
      "====> Test set loss: 99.8044\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 97.044197\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 100.188904\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 98.818626\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 102.555046\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 99.073936\n",
      "====> Epoch: 34 Average loss: 98.1827\n",
      "====> Test set loss: 99.7127\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 98.594040\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 97.191635\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 94.501511\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 98.690399\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 103.872368\n",
      "====> Epoch: 35 Average loss: 98.1179\n",
      "====> Test set loss: 99.6649\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 95.087410\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 98.234856\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 95.805389\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 96.833679\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 98.428703\n",
      "====> Epoch: 36 Average loss: 97.9904\n",
      "====> Test set loss: 99.3304\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 98.872032\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 101.161125\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 99.155075\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 95.180321\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 97.502068\n",
      "====> Epoch: 37 Average loss: 97.9021\n",
      "====> Test set loss: 99.5451\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 98.288452\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 96.451675\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 91.869125\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 100.480011\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 94.503319\n",
      "====> Epoch: 38 Average loss: 97.8004\n",
      "====> Test set loss: 99.6752\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 97.733276\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 96.941788\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 99.450333\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 96.393921\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 97.432587\n",
      "====> Epoch: 39 Average loss: 97.7582\n",
      "====> Test set loss: 99.3283\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 102.163216\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 99.138054\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 98.006294\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 97.932533\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 100.328514\n",
      "====> Epoch: 40 Average loss: 97.6346\n",
      "====> Test set loss: 99.2445\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 97.025574\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 95.550400\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 95.516632\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 96.486748\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 98.864761\n",
      "====> Epoch: 41 Average loss: 97.6254\n",
      "====> Test set loss: 99.3326\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 101.721298\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 94.266357\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 97.971291\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 96.310074\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 94.564148\n",
      "====> Epoch: 42 Average loss: 97.4885\n",
      "====> Test set loss: 99.2837\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 101.479141\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 99.994179\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 99.347214\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 98.107727\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 98.742767\n",
      "====> Epoch: 43 Average loss: 97.4153\n",
      "====> Test set loss: 99.1589\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 96.587646\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 98.342010\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 94.916725\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 96.211853\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 95.477722\n",
      "====> Epoch: 44 Average loss: 97.4059\n",
      "====> Test set loss: 99.0377\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 93.880363\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 95.696350\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 97.928131\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 98.205444\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 98.675232\n",
      "====> Epoch: 45 Average loss: 97.3644\n",
      "====> Test set loss: 99.0928\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 93.576035\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 101.157867\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 97.142761\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 92.374252\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 98.984871\n",
      "====> Epoch: 46 Average loss: 97.2839\n",
      "====> Test set loss: 99.2675\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 96.332794\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 94.261688\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 95.028969\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 100.782364\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 99.160942\n",
      "====> Epoch: 47 Average loss: 97.1678\n",
      "====> Test set loss: 99.1092\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 97.770203\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 92.362877\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 94.995087\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 99.202080\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 96.278229\n",
      "====> Epoch: 48 Average loss: 97.1233\n",
      "====> Test set loss: 99.3799\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 97.824402\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 97.199173\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 98.037933\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 92.411987\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 101.832489\n",
      "====> Epoch: 49 Average loss: 97.0345\n",
      "====> Test set loss: 99.4397\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 97.021126\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 96.290726\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 97.775024\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 101.242371\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 95.552803\n",
      "====> Epoch: 50 Average loss: 96.9911\n",
      "====> Test set loss: 98.9238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "weight = 1.0\n",
    "\n",
    "for epoch in tqdm(range(1, 51)):\n",
    "    train(epoch, device, weight)\n",
    "    test(device, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "    # YOUR CODE!!\n",
    "    z = torch.randn(64, 50).to(device)\n",
    "    sample = vae.decoder(z).to(device)\n",
    "    \n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "            \n",
    "    #z = torch.tensor(z).to(device)\n",
    "    #sample = vae.decoder(z.float())\n",
    "    \n",
    "    if not os.path.exists('./samples'):\n",
    "        os.makedirs('./samples')\n",
    "    \n",
    "    save_image(sample.view(64, 1, 28, 28), './samples/problem 2/sample_50D' + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
