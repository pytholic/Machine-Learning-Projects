{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you'll learn how to model the sentences with recurrent neural networks(RNNs). We'll provide you with basic skeleton codes for preprocessing sequences and performing sentimental analysis with RNNs. However, provided codes can be improved with some simple modifications. The purpose of this homework is to implement several advanced techniques for improving the performance of vanilla RNNs.\n",
    "\n",
    "First, we'll import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchtext\n",
    "!pip install spacy\n",
    "!python -m spacy download en\n",
    "import random\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "For your convenience, we will provide you with the basic preprocessing steps for handling IMDB movie dataset. For more information, see https://pytorch.org/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', include_lengths=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state=random.seed(1234))\n",
    "\n",
    "print('Number of training examples: {:d}'.format(len(train_data)))\n",
    "print('NUmber of validation examples: {:d}'.format(len(valid_data)))\n",
    "print('Number of testing examples: {:d}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data,\n",
    "                 max_size=25000)\n",
    "LABEL.build_vocab(train_data)\n",
    "# Tokens include <unk> and <pad>\n",
    "print('Unique tokens in text vocabulary: {:d}'.format(len(TEXT.vocab)))\n",
    "# Label is either positive or negative\n",
    "print('Unique tokens in label vocabulary: {:d}'.format(len(LABEL.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=False,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the sequence is padded with <PAD>(=1) tokens after the sequence ends.\n",
    "for batch in train_iterator:\n",
    "    text, text_length = batch.text\n",
    "    break\n",
    "\n",
    "print(text[:, -1])\n",
    "print(text[-10:, -1])\n",
    "print(text_length[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will re-load dataset since we already loaded one batch in above cell.\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "We will provide you with skeleton codes for training RNNs below. Run this code and you'll notice that the training / validation performance is not better than random guessing (50\\~60%).\n",
    "In this homework, you'll have to improve the performance of this network above 80% with several techniques commonly used in RNNs. **Please provide your answer in your report and attach notebook file which contains source code for below techniques.**\n",
    "\n",
    "(a) (3pt) Look at the shape of tensor `hidden` and `embedded`. Have you noticed what is the problem? Explain what is the issue and report the test performance when you fix the issue. (Hint: This is related to the length of sequences. See how sequence is padded. You may use `nn.utils.rnn.pack_padded_sequence`.)\n",
    "\n",
    "(b) (3pt) Use different architectures, such as LSTM or GRU, and report the test performance. \"Do not\" change hyperparameters from (a), such as batch_size, hidden_dim,...\n",
    "\n",
    "Now, try to use below techniques to further improve the performance of provided source codes. Compare the test performance of each component with/without it.\n",
    "\n",
    "(c) (1pt) For now, the number of layers in RNN is 1. Try to stack more layers, up to 3.\n",
    "\n",
    "(d) (1pt) Use bidirectional RNNs.\n",
    "\n",
    "(e) (1pt) Use dropout for regularization with stacked layers (recommended: 3 layers and dropout rate 0.5).\n",
    "\n",
    "(f) (1pt) Finally, apply all techniques and have an enough time to play with introduced techniques (e.g., changing hyperparameters, train more epochs, try other techniques you know, ...). Report the final test performance with your implementation and hyperparameter choice. Please note that this is not a competition assignment. We will not evaluate your assignment strictly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim,\n",
    "                           hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        hidden = hidden[-1]\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(TEXT.vocab)\n",
    "embedding_dim = 100 \n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "num_epochs = 10\n",
    "val_iter = 1\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = SimpleRNN(input_dim, embedding_dim, hidden_dim, output_dim, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in train_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths).squeeze(-1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc.item()\n",
    "        \n",
    "    running_loss /= len(train_iterator)\n",
    "    running_acc /= len(train_iterator)\n",
    "    \n",
    "    if epoch % val_iter == 0:\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_iterator:\n",
    "                text, text_lengths = batch.text\n",
    "                eval_predictions = model(text, text_lengths).squeeze(1)\n",
    "                valid_loss += criterion(eval_predictions, batch.label).item()\n",
    "                valid_acc += binary_accuracy(eval_predictions, batch.label).item()\n",
    "                \n",
    "        model.train()\n",
    "        valid_loss /= len(valid_iterator)\n",
    "        valid_acc /= len(valid_iterator)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), './simplernn.pth')  \n",
    "        \n",
    "    training_time = time.time() - start_time\n",
    "    print('#####################################')\n",
    "    print('Epoch {:d} | Training Time {:.1f}s'.format(epoch+1, training_time))\n",
    "    print('Train Loss: {:.4f}, Train Acc: {:.2f}%'.format(running_loss, running_acc*100))\n",
    "    if epoch % val_iter == 0:\n",
    "        print('Valid Loss: {:.4f}, Valid Acc: {:.2f}%'.format(valid_loss, valid_acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS THE TEST PERFORMANCE YOU SHOULD REPORT ##\n",
    "\n",
    "model.load_state_dict(torch.load('./simplernn.pth'))\n",
    "model.eval()\n",
    "test_loss, test_acc = 0, 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        test_preds = model(text, text_lengths).squeeze(1)\n",
    "        test_loss += criterion(test_preds, batch.label).item()\n",
    "        test_acc += binary_accuracy(test_preds, batch.label).item()\n",
    "    test_loss /= len(test_iterator)\n",
    "    test_acc /= len(test_iterator)\n",
    "print('Test Loss: {:.4f}, Test Acc: {:.2f}%'.format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
