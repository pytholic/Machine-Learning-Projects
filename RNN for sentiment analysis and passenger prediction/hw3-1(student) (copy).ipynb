{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you'll learn how to model the sentences with recurrent neural networks(RNNs). We'll provide you with basic skeleton codes for preprocessing sequences and performing sentimental analysis with RNNs. However, provided codes can be improved with some simple modifications. The purpose of this homework is to implement several advanced techniques for improving the performance of vanilla RNNs.\n",
    "\n",
    "First, we'll import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 109 kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/trojan/.local/lib/python3.6/site-packages (from torchtext) (1.4.0)\n",
      "Requirement already satisfied: numpy in /home/trojan/.local/lib/python3.6/site-packages (from torchtext) (1.18.1)\n",
      "Requirement already satisfied: tqdm in /home/trojan/.local/lib/python3.6/site-packages (from torchtext) (4.41.1)\n",
      "Requirement already satisfied: requests in /home/trojan/.local/lib/python3.6/site-packages (from torchtext) (2.22.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 41 kB/s eta 0:00:014\n",
      "\u001b[?25hRequirement already satisfied: six in /home/trojan/.local/lib/python3.6/site-packages (from torchtext) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchtext) (2018.1.18)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchtext) (2.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchtext) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->torchtext) (3.0.4)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.90 torchtext-0.6.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-2.2.4-cp36-cp36m-manylinux1_x86_64.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 51 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (19 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 23 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/trojan/.local/lib/python3.6/site-packages (from spacy) (4.41.1)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting thinc==7.4.0\n",
      "  Downloading thinc-7.4.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 38 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/trojan/.local/lib/python3.6/site-packages (from spacy) (45.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/trojan/.local/lib/python3.6/site-packages (from spacy) (2.22.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (32 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.6.0-py3-none-any.whl (20 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (185 kB)\n",
      "\u001b[K     |████████████████████████████████| 185 kB 35 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 53 kB/s eta 0:00:013\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /home/trojan/.local/lib/python3.6/site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.1.18)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/trojan/.local/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/trojan/.local/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (2.0.1)\n",
      "Requirement already satisfied: more-itertools in /home/trojan/.local/lib/python3.6/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (8.1.0)\n",
      "Installing collected packages: murmurhash, cymem, preshed, plac, wasabi, srsly, blis, catalogue, thinc, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.2.4 srsly-1.0.2 thinc-7.4.0 wasabi-0.6.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "/usr/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext\n",
    "!pip install spacy\n",
    "!python -m spacy download en\n",
    "import random\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "For your convenience, we will provide you with the basic preprocessing steps for handling IMDB movie dataset. For more information, see https://pytorch.org/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:45<00:00, 1.85MB/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy', include_lengths=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "NUmber of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = train_data.split(random_state=random.seed(1234))\n",
    "\n",
    "print('Number of training examples: {:d}'.format(len(train_data)))\n",
    "print('NUmber of validation examples: {:d}'.format(len(valid_data)))\n",
    "print('Number of testing examples: {:d}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in text vocabulary: 25002\n",
      "Unique tokens in label vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data,\n",
    "                 max_size=25000)\n",
    "LABEL.build_vocab(train_data)\n",
    "# Tokens include <unk> and <pad>\n",
    "print('Unique tokens in text vocabulary: {:d}'.format(len(TEXT.vocab)))\n",
    "# Label is either positive or negative\n",
    "print('Unique tokens in label vocabulary: {:d}'.format(len(LABEL.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=False,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  496,  1840,   112,   409,    25, 14770,     9,     3,    31,   311,\n",
      "           34,     8,   926,  5444,     5,  5444,     3,    48,  7793,    12,\n",
      "            8,   151,    13,    12,     9,     6, 17528,     8,    98,     0,\n",
      "           18,     0, 10694,  1914,     0,    14,   152,    34,    91,     2,\n",
      "          467,     7,    16,   386,     4,  3809,     3,   961,    32,   432,\n",
      "            8,     0,    78,   177,    22,     6,   302,     7,  1132,   425,\n",
      "            5,    74,    17,  1061,    13,     0,     2,    74,     0,    18,\n",
      "         5258,     9,  1381,  6347,     2,   388,     5,   416,    22,     2,\n",
      "         9029,  7752,     6,    14,   425,    14,    58,   574,     9,    12,\n",
      "            0,     2,    74,   954,    58,   586,   286,    67,    15,   109,\n",
      "            2,    14,   836,    41,  1609,  1427,    14,  1941,     4,  2642,\n",
      "           12,    15,   429,  3151,     8,    55,   682,     0,    18,   197,\n",
      "          530,    12,    15,   117,    89,   177,     8,    57,    60, 17358,\n",
      "           18,  2429,     3,    13,   104,    12,   117,    89,   177,    23,\n",
      "          105,   101,     8,   127,    25, 14770,    71,     0,    20,    38,\n",
      "            7,     2,   105,   135,    17,  6231,     5,     0,   292,     7,\n",
      "           42,    68,     4,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1], device='cuda:0')\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "tensor(163, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Note that the sequence is padded with <PAD>(=1) tokens after the sequence ends.\n",
    "for batch in train_iterator:\n",
    "    text, text_length = batch.text\n",
    "    break\n",
    "\n",
    "print(text[:, -1])\n",
    "print(text[-10:, -1])\n",
    "print(text_length[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will re-load dataset since we already loaded one batch in above cell.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "We will provide you with skeleton codes for training RNNs below. Run this code and you'll notice that the training / validation performance is not better than random guessing (50\\~60%).\n",
    "In this homework, you'll have to improve the performance of this network above 80% with several techniques commonly used in RNNs. **Please provide your answer in your report and attach notebook file which contains source code for below techniques.**\n",
    "\n",
    "(a) (3pt) Look at the shape of tensor `hidden` and `embedded`. Have you noticed what is the problem? Explain what is the issue and report the test performance when you fix the issue. (Hint: This is related to the length of sequences. See how sequence is padded. You may use `nn.utils.rnn.pack_padded_sequence`.)\n",
    "\n",
    "(b) (3pt) Use different architectures, such as LSTM or GRU, and report the test performance. \"Do not\" change hyperparameters from (a), such as batch_size, hidden_dim,...\n",
    "\n",
    "Now, try to use below techniques to further improve the performance of provided source codes. Compare the test performance of each component with/without it.\n",
    "\n",
    "(c) (1pt) For now, the number of layers in RNN is 1. Try to stack more layers, up to 3.\n",
    "\n",
    "(d) (1pt) Use bidirectional RNNs.\n",
    "\n",
    "(e) (1pt) Use dropout for regularization with stacked layers (recommended: 3 layers and dropout rate 0.5).\n",
    "\n",
    "(f) (1pt) Finally, apply all techniques and have an enough time to play with introduced techniques (e.g., changing hyperparameters, train more epochs, try other techniques you know, ...). Report the final test performance with your implementation and hyperparameter choice. Please note that this is not a competition assignment. We will not evaluate your assignment strictly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim,\n",
    "                           hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        hidden = hidden[-1]\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        packed_embed = pack_padded_sequence(text, input_dim, batch_first=False)\n",
    "        self.rnn = nn.RNN(packed_embed,\n",
    "                           hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        hidden = hidden[-1]\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-9a58f1a0d9c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpad_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-59298e9c5db7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, embedding_dim, hidden_dim, output_dim, pad_idx)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSimpleRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpacked_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         self.rnn = nn.RNN(packed_embed,\n\u001b[1;32m      9\u001b[0m                            hidden_dim)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "input_dim = len(TEXT.vocab)\n",
    "embedding_dim = 100 \n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "num_epochs = 10\n",
    "val_iter = 1\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = SimpleRNN(input_dim, embedding_dim, hidden_dim, output_dim, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "Epoch 1 | Training Time 4.3s\n",
      "Train Loss: 0.6857, Train Acc: 54.94%\n",
      "Valid Loss: 0.6699, Valid Acc: 58.28%\n",
      "#####################################\n",
      "Epoch 2 | Training Time 3.7s\n",
      "Train Loss: 0.6725, Train Acc: 58.26%\n",
      "Valid Loss: 0.6872, Valid Acc: 53.72%\n",
      "#####################################\n",
      "Epoch 3 | Training Time 4.0s\n",
      "Train Loss: 0.6440, Train Acc: 64.02%\n",
      "Valid Loss: 0.6321, Valid Acc: 65.53%\n",
      "#####################################\n",
      "Epoch 4 | Training Time 4.1s\n",
      "Train Loss: 0.6219, Train Acc: 66.59%\n",
      "Valid Loss: 0.6237, Valid Acc: 66.48%\n",
      "#####################################\n",
      "Epoch 5 | Training Time 3.8s\n",
      "Train Loss: 0.6554, Train Acc: 58.66%\n",
      "Valid Loss: 0.6929, Valid Acc: 51.76%\n",
      "#####################################\n",
      "Epoch 6 | Training Time 3.7s\n",
      "Train Loss: 0.6882, Train Acc: 53.77%\n",
      "Valid Loss: 0.6630, Valid Acc: 58.79%\n",
      "#####################################\n",
      "Epoch 7 | Training Time 3.8s\n",
      "Train Loss: 0.6661, Train Acc: 58.24%\n",
      "Valid Loss: 0.6891, Valid Acc: 51.81%\n",
      "#####################################\n",
      "Epoch 8 | Training Time 3.7s\n",
      "Train Loss: 0.6905, Train Acc: 53.04%\n",
      "Valid Loss: 0.6921, Valid Acc: 52.03%\n",
      "#####################################\n",
      "Epoch 9 | Training Time 3.7s\n",
      "Train Loss: 0.6914, Train Acc: 52.64%\n",
      "Valid Loss: 0.6904, Valid Acc: 52.53%\n",
      "#####################################\n",
      "Epoch 10 | Training Time 3.8s\n",
      "Train Loss: 0.6981, Train Acc: 55.24%\n",
      "Valid Loss: 0.7178, Valid Acc: 56.85%\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch in train_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        predictions = model(text, text_lengths).squeeze(-1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc.item()\n",
    "        \n",
    "    running_loss /= len(train_iterator)\n",
    "    running_acc /= len(train_iterator)\n",
    "    \n",
    "    if epoch % val_iter == 0:\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in valid_iterator:\n",
    "                text, text_lengths = batch.text\n",
    "                eval_predictions = model(text, text_lengths).squeeze(1)\n",
    "                valid_loss += criterion(eval_predictions, batch.label).item()\n",
    "                valid_acc += binary_accuracy(eval_predictions, batch.label).item()\n",
    "                \n",
    "        model.train()\n",
    "        valid_loss /= len(valid_iterator)\n",
    "        valid_acc /= len(valid_iterator)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), './simplernn.pth')  \n",
    "        \n",
    "    training_time = time.time() - start_time\n",
    "    print('#####################################')\n",
    "    print('Epoch {:d} | Training Time {:.1f}s'.format(epoch+1, training_time))\n",
    "    print('Train Loss: {:.4f}, Train Acc: {:.2f}%'.format(running_loss, running_acc*100))\n",
    "    if epoch % val_iter == 0:\n",
    "        print('Valid Loss: {:.4f}, Valid Acc: {:.2f}%'.format(valid_loss, valid_acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6137, Test Acc: 67.14%\n"
     ]
    }
   ],
   "source": [
    "## THIS IS THE TEST PERFORMANCE YOU SHOULD REPORT ##\n",
    "\n",
    "model.load_state_dict(torch.load('./simplernn.pth'))\n",
    "model.eval()\n",
    "test_loss, test_acc = 0, 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        test_preds = model(text, text_lengths).squeeze(1)\n",
    "        test_loss += criterion(test_preds, batch.label).item()\n",
    "        test_acc += binary_accuracy(test_preds, batch.label).item()\n",
    "    test_loss /= len(test_iterator)\n",
    "    test_acc /= len(test_iterator)\n",
    "print('Test Loss: {:.4f}, Test Acc: {:.2f}%'.format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
