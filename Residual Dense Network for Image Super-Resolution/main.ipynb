{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Super-Resolution\n",
    "#HW1\n",
    "#EE838A\n",
    "#by Raja Haseeb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from math import log10\n",
    "\n",
    "from model import model\n",
    "from data import DIV2K, Set5\n",
    "from utils import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if gpu is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using easydict instead of argparser because I am using notebook\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "args = edict()\n",
    "\n",
    "#Directory paths\n",
    "args.dataDir = '/home/trojan/Desktop/Image restoration/Homeworks/HW1/Dataset/SR_data/train'   #dataset directory\n",
    "args.saveDir = '/home/trojan/Desktop/Image restoration/Homeworks/HW1/result'   #datasave directory\n",
    "args.HR_valDataroot = '/home/trojan/Desktop/Image restoration/Homeworks/HW1/Dataset/SR_data/benchmark/Set5/HR'\n",
    "args.LR_valDataroot = '/home/trojan/Desktop/Image restoration/Homeworks/HW1/Dataset/SR_data/benchmark/Set5/LR_bicubic/X2'\n",
    "args.valBatchSize = 5\n",
    "\n",
    "#Basic options\n",
    "args.load = 'NetFinal'   #save result\n",
    "args.model_name = 'RDN_model'   #model to select\n",
    "args.finetuning = False   #fintuning the training\n",
    "args.need_patch = True   #get patch from image\n",
    "\n",
    "#Network options\n",
    "args.num_dense_layer = 3   #number of dense blocks\n",
    "args.growth_rate = 32   #growth rate of dense net\n",
    "args.num_features = 64   #number of feathure maps\n",
    "args.num_channel = 3   #number of color maps to use\n",
    "args.patch_size = 64   #patch size (GT)\n",
    "\n",
    "args.nThreads = 8   #number of threads for data loading\n",
    "args.batch_size = 16 #batch size for training\n",
    "args.lr = 1e-3   #learning rate\n",
    "args.epochs = 200   #number of training epochs\n",
    "args.lr_decay = 100   #number of epochs to drop lr\n",
    "args.decay_type = 'inv' #lr decay type\n",
    "args.loss_type = 'L1'   #Loss type\n",
    "\n",
    "args.period = 10   #period of evaluation\n",
    "args.scale = 2   #scale output size /input size\n",
    "args.gpu = True   #gpu index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Settings\n",
    "if args.gpu == 0:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "elif args.gpu == 1:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Check cuda device\n",
    "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "\n",
    "def get_dataset(args):\n",
    "    data_train = DIV2K(args)\n",
    "    dataloader = torch.utils.data.DataLoader(data_train, batch_size=args.batch_size,\n",
    "                                             drop_last=True, shuffle=True, num_workers=int(args.nThreads), pin_memory=False)\n",
    "    return dataloader\n",
    "\n",
    "def get_testdataset(args):\n",
    "    data_test = Set5(args)\n",
    "    dataloader = torch.utils.data.DataLoader(data_test, batch_size=1,\n",
    "                                             drop_last=True, shuffle=False, num_workers=int(args.nThreads), pin_memory=False)\n",
    "    return dataloader\n",
    "\n",
    "def set_loss(args):\n",
    "    loss_type = args.loss_type\n",
    "    if loss_type == 'MSE':\n",
    "        lossfunction = nn.MSELoss()\n",
    "    elif loss_type == 'L1':\n",
    "        lossfunction = nn.L1Loss()\n",
    "    return lossfunction\n",
    "\n",
    "\n",
    "def set_lr(args, epoch, optimizer):\n",
    "    lr_decay = args.lr_decay\n",
    "    decay_type = args.decay_type\n",
    "    if decay_type == 'step':\n",
    "        epoch_iter = (epoch + 1) // lr_decay\n",
    "        lr = args.lr / 2 ** epoch_iter\n",
    "    elif decay_type == 'exp':\n",
    "        k = math.log(2) / lr_decay\n",
    "        lr = args.lr * math.exp(-k * epoch)\n",
    "    elif decay_type == 'inv':\n",
    "        k = 1 / lr_decay\n",
    "        lr = args.lr / (1 + k * epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def test(args, model, dataloader):\n",
    "\n",
    "    avg_psnr = 0\n",
    "    psnr_val = 0\n",
    "    n = 0\n",
    "    for batch, (im_lr, im_hr) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            im_lr = Variable(im_lr.cuda(), volatile=False)\n",
    "            im_hr = Variable(im_hr.cuda())\n",
    "            output = model(im_lr)\n",
    "\n",
    "        output = output.cpu()\n",
    "        output = output.data.squeeze(0)\n",
    "        \n",
    "        # denormalization\n",
    "        mean = [0.5, 0.5, 0.5]\n",
    "        std = [0.5, 0.5, 0.5]\n",
    "        for t, m, s in zip(output, mean, std):\n",
    "            t.mul_(s).add_(m)\n",
    "\n",
    "        output = output.numpy()\n",
    "        output *= 255.0\n",
    "        output = output.clip(0, 255)\n",
    "        \n",
    "        #added code to save the resulting images\n",
    "        output_new = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))\n",
    "        RGB_output = cv2.cvtColor(output_new, cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(args.saveDir + '/image_{}.png'.format(n), RGB_output)\n",
    "        n += 1\n",
    "        #output = Image.fromarray(np.uint8(output[0]), mode='RGB')\n",
    "        #plt.imshow(output)\n",
    "\n",
    "        # =========== Target Image ===============\n",
    "        im_hr = im_hr.cpu()\n",
    "        im_hr = im_hr.data.squeeze(0)\n",
    "        mean = [0.5, 0.5, 0.5]\n",
    "        std = [0.5, 0.5, 0.5]\n",
    "        for t, m, s in zip(im_hr, mean, std):\n",
    "            t.mul_(s).add_(m)\n",
    "\n",
    "        im_hr = im_hr.numpy()\n",
    "        im_hr *= 255.0\n",
    "        im_hr = im_hr.clip(0, 255)\n",
    "        # im_hr = Image.fromarray(np.uint8(im_hr[0]), mode='RGB')\n",
    "\n",
    "        mse = ((im_hr[:, 8:-8,8:-8] - output[:, 8:-8,8:-8]) ** 2).mean()\n",
    "        psnr = 10 * log10(255 * 255 / (mse + 10 ** (-10)))\n",
    "        psnr_val = psnr\n",
    "        avg_psnr += psnr\n",
    "        \n",
    "        \n",
    "\n",
    "    return avg_psnr/args.valBatchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # Set a Model\n",
    "    if args.model_name == 'RDN_model':\n",
    "        my_model = model.RDN_model(args)\n",
    "    my_model.apply(weights_init)\n",
    "    my_model.cuda()\n",
    "\n",
    "    save = saveData(args)\n",
    "\n",
    "    Numparams = count_parameters(my_model)\n",
    "    save.save_log(str(Numparams))\n",
    "\n",
    "    last_epoch = 0\n",
    "    # fine-tuning or retrain\n",
    "    if args.finetuning:\n",
    "        my_model, last_epoch = save.load_model(my_model)\n",
    "\n",
    "    # load data\n",
    "    dataloader = get_dataset(args) # [-1,1]\n",
    "    testdataloader = get_testdataset(args)\n",
    "\n",
    "    start_epoch = last_epoch\n",
    "    lossfunction = set_loss(args)\n",
    "    lossfunction.cuda()\n",
    "    total_loss = 0\n",
    "    total_time = 0\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        start = time.time()\n",
    "        optimizer = optim.Adam(my_model.parameters()) # optimizer\n",
    "        learning_rate = set_lr(args, epoch, optimizer)\n",
    "        total_loss_ = 0\n",
    "        loss_ = 0\n",
    "        for batch, (im_lr, im_hr) in enumerate(dataloader):\n",
    "            im_lr = Variable(im_lr.cuda())\n",
    "            im_hr = Variable(im_hr.cuda())\n",
    "\n",
    "            my_model.zero_grad()\n",
    "            output = my_model(im_lr)\n",
    "            loss = lossfunction(output, im_hr)\n",
    "            total_loss = loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_ += loss.data.cpu().numpy()\n",
    "            total_loss_ += loss.data.cpu().numpy()\n",
    "        loss_ = loss_ / (batch + 1)\n",
    "        total_loss_ = total_loss_ / (batch + 1)\n",
    "\n",
    "        end = time.time()\n",
    "        epoch_time = (end - start)\n",
    "        total_time = total_time + epoch_time\n",
    "            \n",
    "        if (epoch + 1) % args.period == 0:\n",
    "            my_model.eval()\n",
    "            avg_psnr = test(args, my_model, testdataloader)\n",
    "            my_model.train()\n",
    "            log = \"[{} / {}] \\tLearning_rate: {:.5f}\\t Train total_loss: {:.4f}\\t Train Loss: {:.4f} \\t Val PSNR: {:.4f} Time: {:.4f}\".format(epoch + 1,\n",
    "                                                                                                                                              args.epochs, learning_rate, total_loss_, loss_, avg_psnr, total_time)\n",
    "            print(log)\n",
    "            save.save_log(log)\n",
    "            save.save_model(my_model, epoch)\n",
    "            total_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 / 200] \tLearning_rate: 0.00092\t Train total_loss: 0.0338\t Train Loss: 0.0338 \t Val PSNR: 34.4527 Time: 53.6844\n",
      "[20 / 200] \tLearning_rate: 0.00084\t Train total_loss: 0.0324\t Train Loss: 0.0324 \t Val PSNR: 34.6662 Time: 54.6039\n",
      "[30 / 200] \tLearning_rate: 0.00078\t Train total_loss: 0.0306\t Train Loss: 0.0306 \t Val PSNR: 34.9674 Time: 55.0322\n",
      "[40 / 200] \tLearning_rate: 0.00072\t Train total_loss: 0.0302\t Train Loss: 0.0302 \t Val PSNR: 34.8863 Time: 55.5991\n",
      "[50 / 200] \tLearning_rate: 0.00067\t Train total_loss: 0.0294\t Train Loss: 0.0294 \t Val PSNR: 35.1536 Time: 55.4029\n",
      "[60 / 200] \tLearning_rate: 0.00063\t Train total_loss: 0.0297\t Train Loss: 0.0297 \t Val PSNR: 35.1597 Time: 56.3667\n",
      "[70 / 200] \tLearning_rate: 0.00059\t Train total_loss: 0.0302\t Train Loss: 0.0302 \t Val PSNR: 35.1325 Time: 56.4092\n",
      "[80 / 200] \tLearning_rate: 0.00056\t Train total_loss: 0.0294\t Train Loss: 0.0294 \t Val PSNR: 35.2028 Time: 55.5190\n",
      "[90 / 200] \tLearning_rate: 0.00053\t Train total_loss: 0.0287\t Train Loss: 0.0287 \t Val PSNR: 35.1290 Time: 55.5283\n",
      "[100 / 200] \tLearning_rate: 0.00050\t Train total_loss: 0.0290\t Train Loss: 0.0290 \t Val PSNR: 35.1750 Time: 55.4785\n",
      "[110 / 200] \tLearning_rate: 0.00048\t Train total_loss: 0.0284\t Train Loss: 0.0284 \t Val PSNR: 35.1149 Time: 55.5511\n",
      "[120 / 200] \tLearning_rate: 0.00046\t Train total_loss: 0.0284\t Train Loss: 0.0284 \t Val PSNR: 35.2504 Time: 55.6617\n",
      "[130 / 200] \tLearning_rate: 0.00044\t Train total_loss: 0.0284\t Train Loss: 0.0284 \t Val PSNR: 35.2317 Time: 55.8889\n",
      "[140 / 200] \tLearning_rate: 0.00042\t Train total_loss: 0.0284\t Train Loss: 0.0284 \t Val PSNR: 35.2949 Time: 55.7293\n",
      "[150 / 200] \tLearning_rate: 0.00040\t Train total_loss: 0.0286\t Train Loss: 0.0286 \t Val PSNR: 35.2324 Time: 55.7819\n",
      "[160 / 200] \tLearning_rate: 0.00039\t Train total_loss: 0.0278\t Train Loss: 0.0278 \t Val PSNR: 35.3353 Time: 55.7506\n",
      "[170 / 200] \tLearning_rate: 0.00037\t Train total_loss: 0.0281\t Train Loss: 0.0281 \t Val PSNR: 35.3144 Time: 55.7757\n",
      "[180 / 200] \tLearning_rate: 0.00036\t Train total_loss: 0.0284\t Train Loss: 0.0284 \t Val PSNR: 35.2243 Time: 55.8601\n",
      "[190 / 200] \tLearning_rate: 0.00035\t Train total_loss: 0.0279\t Train Loss: 0.0279 \t Val PSNR: 35.3753 Time: 55.9344\n",
      "[200 / 200] \tLearning_rate: 0.00033\t Train total_loss: 0.0284\t Train Loss: 0.0284 \t Val PSNR: 35.2533 Time: 55.9750\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RDN_model(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (RDB1): RDB(\n",
       "    (layer_dense): Sequential(\n",
       "      (0): create_dense_layer(\n",
       "        (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): create_dense_layer(\n",
       "        (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): create_dense_layer(\n",
       "        (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (conv_1x1): Conv2d(160, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (RDB2): RDB(\n",
       "    (layer_dense): Sequential(\n",
       "      (0): create_dense_layer(\n",
       "        (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): create_dense_layer(\n",
       "        (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): create_dense_layer(\n",
       "        (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (conv_1x1): Conv2d(160, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (RDB3): RDB(\n",
       "    (layer_dense): Sequential(\n",
       "      (0): create_dense_layer(\n",
       "        (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): create_dense_layer(\n",
       "        (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): create_dense_layer(\n",
       "        (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (conv_1x1): Conv2d(160, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (GFF_1x1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (GFF_3x3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_up): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upsample): conv_sub_pixel(\n",
       "    (sub_pixel_layer): Sequential(\n",
       "      (0): PixelShuffle(upscale_factor=2)\n",
       "    )\n",
       "  )\n",
       "  (conv3): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the trained model\n",
    "\n",
    "my_model = model.RDN_model(args)\n",
    "my_model.load_state_dict(torch.load(args.saveDir + '/NetFinal/model/model_lastest.pt'))\n",
    "my_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.25326338005327"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the model\n",
    "test(args, my_model, dataloader = get_testdataset(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
